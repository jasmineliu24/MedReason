{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_gpus: 2\n",
      "CUDA_VISIBLE_DEVICES: 2,3\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2,3\"\n",
    "# \"0,1,2,3\"\n",
    "num_gpus = len(os.environ[\"CUDA_VISIBLE_DEVICES\"].split(\",\"))\n",
    "print(\"num_gpus:\", num_gpus)\n",
    "print(\"CUDA_VISIBLE_DEVICES:\", os.environ[\"CUDA_VISIBLE_DEVICES\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "# import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/home13/jasmineliu/.conda/envs/kure_new/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-30 22:31:50 [__init__.py:243] No platform detected, vLLM is running on UnspecifiedPlatform\n",
      "WARNING 04-30 22:31:50 [_custom_ops.py:21] Failed to import from vllm._C with ImportError('libcuda.so.1: cannot open shared object file: No such file or directory')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-30 22:31:52,432\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from vllm import LLM, SamplingParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset.process import format_chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed everything: 42\n"
     ]
    }
   ],
   "source": [
    "def seed_everything(seed):\n",
    "    seed = int(seed)\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    transformers.set_seed(seed)\n",
    "    print(f\"seed everything: {seed}\")\n",
    "\n",
    "seed = 42\n",
    "seed_everything(seed=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name of model\n",
    "with open(\"dict_model_path.json\", \"r\") as f:\n",
    "    dict_model_path = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'Deepseek8B'\n",
    "# path_dir_model = dict_model_path[model_name]\n",
    "path_dir_model = \"/n/holylfs06/LABS/kempner_undergrads/Lab/jasmineliu/DeepSeek-R1-Distill-Llama-8B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # loading tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(path_dir_model, padding_side='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmptyArgs:\n",
    "    def __init__(self):\n",
    "        self.batch_size = 32\n",
    "        self.gpus = os.environ[\"CUDA_VISIBLE_DEVICES\"].split(\",\")\n",
    "        self.model_path = path_dir_model\n",
    "        self.model_name = model_name\n",
    "\n",
    "args = EmptyArgs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 32,\n",
       " 'gpus': ['2', '3'],\n",
       " 'model_path': '/n/holylfs06/LABS/kempner_undergrads/Lab/jasmineliu/DeepSeek-R1-Distill-Llama-8B',\n",
       " 'model_name': 'Deepseek8B',\n",
       " 'num_workers': 4,\n",
       " 'max_token_all': 102400,\n",
       " 'max_token_output': 3072,\n",
       " 'max_token_input': 99328}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from model.init import load_config\n",
    "\n",
    "load_config(args)\n",
    "args.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_name = \"106.MIMIC-III Outcome.Diagnosis\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset_raw/106.MIMIC-III Outcome.Diagnosis.SFT.json ...\n",
      "The number of data: 1000\n"
     ]
    }
   ],
   "source": [
    "path_file_data = f\"dataset_raw/{task_name}.SFT.json\"\n",
    "print(f\"Loading {path_file_data} ...\")\n",
    "with open(path_file_data, \"r\") as file:\n",
    "    list_dict_data = json.load(file)\n",
    "list_dict_data = [\n",
    "    dict_data for dict_data in list_dict_data if dict_data[\"split\"] == \"test\"\n",
    "]\n",
    "print(f\"The number of data: {len(list_dict_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'task': '106.MIMIC-III Outcome.Dignosis',\n",
       " 'language': 'en',\n",
       " 'type': 'clf',\n",
       " 'id': 34242,\n",
       " 'split': 'test',\n",
       " 'instruction': 'Given a patient\\'s information from admission notes, predict the patient\\'s ICD-9 diagnosis codes, which are the codes used to classify the patient\\'s diagnosis. Each ICD-9 diagnosis code is a 3-digit code that represents a specific diagnosis, and each patient may have multiple diagnosis codes.\\nReturn your answer in the following format. DO NOT GIVE ANY EXPLANATION:\\nICD-9 Diagnosis codes: code 1, code 2, ..., code n\\nThe optional list for \"code\" is ICD-9 diagnosis codes.',\n",
       " 'input': 'CHIEF COMPLAINT: Right foot ulcer getting worse\\n\\nPRESENT ILLNESS: 65 year old male with type 2 diabetes, HTN,\\nHypercholesterolemia, CAD and history of remote stroke, current\\nsmoker, who was brought to the [**Hospital1 882**] ER by his brother\\nsecondary to concern about the state of his R foot.  They\\ntransferred him to [**Hospital1 18**] as he gets all of his podiatric care\\nhere with Dr. [**Last Name (STitle) **].  He has had chronic diabetic foot ulcers\\nof both feet, most recently a persistent ulcer on the plantar\\nsurface of the 1st metatarsal for the last few months.  He has\\nbeen non-weight bearing on his R foot for at least the last\\nmonth, however he admits to walking on it recently.  His brother\\nsays the ulcer has gotten progressively worse, with a foul odor.\\n His R leg has also gotten progressively more swollen over the\\nlast week.  He was scheduled for pre-operative evaluation with\\nDr. [**Last Name (STitle) **] on [**5-22**], with surgery planned for [**5-29**].\\n\\nAt [**Hospital1 882**] his vitals were 102.0, 110, 100/60, 95% on RA.  His\\nlabs were notable for a WBC of 14 with 9% bands, as well as Na\\nof 129, K 5.2, BUN 12, creat 0.6.  He received vancomycin and\\nzosyn, and was seen by surgery who recommended transfer to [**Hospital1 18**]\\nfor probably surgery.\\n\\nIn the ED here his vitals were 101.3, HR 113, 95/palp, 14, 95%\\nRA.  He was given vancomycin and clindamycin.  A L IJ was placed\\nafter unsuccessful attempts at a SC, and code sepsis was called.\\n His CVP was 5.  He received 6L NS in the ED, and was started on\\na levophed drip.  He was seen by podiatry who are planning on\\ntaking him to surgery within the next couple of days pending\\n\"medical stability.\"\\n\\nThe patient denies fevers, but thinks he\\'s had some \"sweats\"\\nover the last week.  He says he\\'s been feeling otherwise well.\\nDenies any lightheadedness, dizziness, chest pain.  He gets\\nshortness of breath when going up a flight of stairs.  At\\nbaseline he says he can walk around without difficulty and go up\\nstairs without difficulty.   He does not exercise.\\n\\nMEDICAL HISTORY: 1) Type 2 DM, with neuropathy, complicated by chronic diabetic\\nfoot ulcers secondary to plantar flexed first metatarsal for\\nwhich he has multiple antibiotic courses.\\n2) HTN\\n3) Hypercholestremia\\n4) CAD\\n5) CVA [**95**] years ago, on ASA and plavix since\\n6) Carotid enderectomy\\n7) Status post cholecystectomy, appendectomy.\\n\\nMEDICATION ON ADMISSION: Plavix\\nASA\\nGlucophage\\nGlyburide\\n\\nALLERGIES: Patient recorded as having No Known Allergies to Drugs\\n\\nAttending:[**First Name3 (LF) 2145**]\\n\\nPHYSICAL EXAM: 100.4, 95, 99/69 MAP 79, CVP 10, RR 22, 94% RA.\\nI/O:  6214/280.\\nGen:  Overweight caucasian male appearing slightly disheveled.\\nConversant.  AAO x 3.  Foul smelling.\\nHEENT:  Dry MM.\\nNeck:  JVP at 10 cm.\\nCor: RR, normal rate, 2/6 systolic murmur at LSB without\\nradiation.\\nLungs:  CTA b/l, no w/r/r.\\nAbd: NABS, soft, large oblique scar in RUQ, vertical scar in\\nRLQ.\\nExtr:  2+ pitting edema of R leg up to mid-tibia.  Erythema of\\nRLE up to ankle.  L foot with strong DP/PT pulses, diminished\\nsensation, no active ulcers.  On plantar aspect of R 1st\\nmetatarsal there is a 2-3 cm ulcer with purulent base, marked\\nsurrounding edema and erythema, and foul smelling odor.\\n\\nFAMILY HISTORY: Mother and father both had DM, both deceased.  Father with MI at\\n60 and PVD.  Mother had a stroke.\\n\\nSOCIAL HISTORY: Patient is separated from his wife for the last 10 years,\\ncurrently living with his brother.  They have 5 children, some\\nof which are in the area.  He says he just quit smoking\\nyesterday, but has smoked 1/2-1 PPD x 50 years.  He used to\\ndrink heavily, about a 6 pack on the weekends, but is in AA and\\nhas been sober x 10 years.  Denies IVDU.',\n",
       " 'output': 'ICD-9 Code: 038, 682, 707, 250, 785, 496, 276, 995, 357, 599, 272, V125, 401, 730'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_dict_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load 106.MIMIC-III Outcome.Diagnosis data: train: 29928, val: 4314, test: 1000\n"
     ]
    }
   ],
   "source": [
    "from dataset.classification import Task_clf_Brain_MRI_AIS\n",
    "task = Task_clf_Brain_MRI_AIS(args=args, task=task_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "task.setup(tokenizer=tokenizer, prompt_mode='direc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose the way to load the model:\n",
    "- Huggingface model loading\n",
    "- Huggingface pipeline\n",
    "- vLLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModel.from_pretrained(path_dir_model, torch_dtype=torch.bfloat16, device_map='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the model to the evaluation mode\n",
    "model.eval()\n",
    "# greedy decoding\n",
    "model.generation_config.do_sample = False\n",
    "model.generation_config.temperature = None\n",
    "model.generation_config.top_k = None\n",
    "model.generation_config.top_p = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "    print(\"Tokenizer: Now pad_token_id is:\", tokenizer.pad_token_id)\n",
    "else:\n",
    "    print(\"Tokenizer: pad_token_id is already set:\", tokenizer.pad_token_id)\n",
    "if model.generation_config.pad_token_id is None:\n",
    "    model.generation_config.pad_token_id = tokenizer.eos_token_id\n",
    "    print(\"Model: Now pad_token_id is:\", model.generation_config.pad_token_id)\n",
    "else:\n",
    "    print(\"Model: pad_token_id is already set:\", model.generation_config.pad_token_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HF Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=path_dir_model, device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_input = \"The medical condition is characterized by\"\n",
    "generated_text = pipe(str_input, num_return_sequences=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### vLLM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to infer device type, please set the environment variable `VLLM_LOGGING_LEVEL=DEBUG` to turn on verbose logging to help debug the issue.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m model = \u001b[43mLLM\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath_dir_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtensor_parallel_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbfloat16\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# max_model_len=4096,\u001b[39;49;00m\n\u001b[32m      6\u001b[39m \u001b[43m            \u001b[49m\u001b[43mgpu_memory_utilization\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.90\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/kure_new/lib/python3.12/site-packages/vllm/utils.py:1161\u001b[39m, in \u001b[36mdeprecate_args.<locals>.wrapper.<locals>.inner\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1154\u001b[39m             msg += \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00madditional_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1156\u001b[39m         warnings.warn(\n\u001b[32m   1157\u001b[39m             \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m(msg),\n\u001b[32m   1158\u001b[39m             stacklevel=\u001b[32m3\u001b[39m,  \u001b[38;5;66;03m# The inner function takes up one level\u001b[39;00m\n\u001b[32m   1159\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1161\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/kure_new/lib/python3.12/site-packages/vllm/entrypoints/llm.py:247\u001b[39m, in \u001b[36mLLM.__init__\u001b[39m\u001b[34m(self, model, tokenizer, tokenizer_mode, skip_tokenizer_init, trust_remote_code, allowed_local_media_path, tensor_parallel_size, dtype, quantization, revision, tokenizer_revision, seed, gpu_memory_utilization, swap_space, cpu_offload_gb, enforce_eager, max_seq_len_to_capture, disable_custom_all_reduce, disable_async_output_proc, hf_token, hf_overrides, mm_processor_kwargs, task, override_pooler_config, compilation_config, **kwargs)\u001b[39m\n\u001b[32m    217\u001b[39m engine_args = EngineArgs(\n\u001b[32m    218\u001b[39m     model=model,\n\u001b[32m    219\u001b[39m     task=task,\n\u001b[32m   (...)\u001b[39m\u001b[32m    243\u001b[39m     **kwargs,\n\u001b[32m    244\u001b[39m )\n\u001b[32m    246\u001b[39m \u001b[38;5;66;03m# Create the Engine (autoselects V0 vs V1)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m247\u001b[39m \u001b[38;5;28mself\u001b[39m.llm_engine = \u001b[43mLLMEngine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_engine_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    248\u001b[39m \u001b[43m    \u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43mUsageContext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLLM_CLASS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    249\u001b[39m \u001b[38;5;28mself\u001b[39m.engine_class = \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m.llm_engine)\n\u001b[32m    251\u001b[39m \u001b[38;5;28mself\u001b[39m.request_counter = Counter()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/kure_new/lib/python3.12/site-packages/vllm/engine/llm_engine.py:503\u001b[39m, in \u001b[36mLLMEngine.from_engine_args\u001b[39m\u001b[34m(cls, engine_args, usage_context, stat_loggers)\u001b[39m\n\u001b[32m    501\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Creates an LLM engine from the engine arguments.\"\"\"\u001b[39;00m\n\u001b[32m    502\u001b[39m \u001b[38;5;66;03m# Create the engine configs.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m503\u001b[39m vllm_config = \u001b[43mengine_args\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_engine_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    505\u001b[39m engine_cls = \u001b[38;5;28mcls\u001b[39m\n\u001b[32m    506\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m envs.VLLM_USE_V1:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/kure_new/lib/python3.12/site-packages/vllm/engine/arg_utils.py:1098\u001b[39m, in \u001b[36mEngineArgs.create_engine_config\u001b[39m\u001b[34m(self, usage_context)\u001b[39m\n\u001b[32m   1095\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvllm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mplatforms\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m current_platform\n\u001b[32m   1096\u001b[39m current_platform.pre_register_and_update()\n\u001b[32m-> \u001b[39m\u001b[32m1098\u001b[39m device_config = \u001b[43mDeviceConfig\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1099\u001b[39m model_config = \u001b[38;5;28mself\u001b[39m.create_model_config()\n\u001b[32m   1101\u001b[39m \u001b[38;5;66;03m# * If VLLM_USE_V1 is unset, we enable V1 for \"supported features\"\u001b[39;00m\n\u001b[32m   1102\u001b[39m \u001b[38;5;66;03m#   and fall back to V0 for experimental or unsupported features.\u001b[39;00m\n\u001b[32m   1103\u001b[39m \u001b[38;5;66;03m# * If VLLM_USE_V1=1, we enable V1 for supported + experimental\u001b[39;00m\n\u001b[32m   1104\u001b[39m \u001b[38;5;66;03m#   features and raise error for unsupported features.\u001b[39;00m\n\u001b[32m   1105\u001b[39m \u001b[38;5;66;03m# * If VLLM_USE_V1=0, we disable V1.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<string>:4\u001b[39m, in \u001b[36m__init__\u001b[39m\u001b[34m(self, device)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/kure_new/lib/python3.12/site-packages/vllm/config.py:2119\u001b[39m, in \u001b[36mDeviceConfig.__post_init__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   2117\u001b[39m     \u001b[38;5;28mself\u001b[39m.device_type = current_platform.device_type\n\u001b[32m   2118\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.device_type:\n\u001b[32m-> \u001b[39m\u001b[32m2119\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   2120\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mFailed to infer device type, please set \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2121\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mthe environment variable `VLLM_LOGGING_LEVEL=DEBUG` \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2122\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mto turn on verbose logging to help debug the issue.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   2123\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2124\u001b[39m     \u001b[38;5;66;03m# Device type is assigned explicitly\u001b[39;00m\n\u001b[32m   2125\u001b[39m     \u001b[38;5;28mself\u001b[39m.device_type = \u001b[38;5;28mself\u001b[39m.device\n",
      "\u001b[31mRuntimeError\u001b[39m: Failed to infer device type, please set the environment variable `VLLM_LOGGING_LEVEL=DEBUG` to turn on verbose logging to help debug the issue."
     ]
    }
   ],
   "source": [
    "model = LLM(\n",
    "            model=path_dir_model,\n",
    "            tensor_parallel_size=2,\n",
    "            dtype=\"bfloat16\",\n",
    "            # max_model_len=4096,\n",
    "            gpu_memory_utilization=0.90,\n",
    "            trust_remote_code=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Deepseek8B ...\n",
      "INFO 04-30 22:33:41 [config.py:717] This model supports multiple tasks: {'generate', 'score', 'reward', 'embed', 'classify'}. Defaulting to 'generate'.\n",
      "WARNING 04-30 22:33:41 [arg_utils.py:1658] device type= is not supported by the V1 Engine. Falling back to V0. \n",
      "WARNING 04-30 22:33:41 [arg_utils.py:1536] The model has a long context length (102400). This may causeOOM during the initial memory profiling phase, or result in low performance due to small KV cache size. Consider setting --max-model-len to a smaller value.\n",
      "INFO 04-30 22:33:41 [config.py:1770] Defaulting to use mp for distributed inference\n",
      "INFO 04-30 22:33:41 [config.py:1804] Disabled the custom all-reduce kernel because it is not supported on current platform.\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotImplementedError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m      6\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLoading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m ...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m     model = \u001b[43mLLM\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath_dir_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_parallel_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_gpus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbfloat16\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_model_len\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmax_token_all\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgpu_memory_utilization\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menforce_eager\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcuda\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/kure_new/lib/python3.12/site-packages/vllm/utils.py:1161\u001b[39m, in \u001b[36mdeprecate_args.<locals>.wrapper.<locals>.inner\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1154\u001b[39m             msg += \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00madditional_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1156\u001b[39m         warnings.warn(\n\u001b[32m   1157\u001b[39m             \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m(msg),\n\u001b[32m   1158\u001b[39m             stacklevel=\u001b[32m3\u001b[39m,  \u001b[38;5;66;03m# The inner function takes up one level\u001b[39;00m\n\u001b[32m   1159\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1161\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/kure_new/lib/python3.12/site-packages/vllm/entrypoints/llm.py:247\u001b[39m, in \u001b[36mLLM.__init__\u001b[39m\u001b[34m(self, model, tokenizer, tokenizer_mode, skip_tokenizer_init, trust_remote_code, allowed_local_media_path, tensor_parallel_size, dtype, quantization, revision, tokenizer_revision, seed, gpu_memory_utilization, swap_space, cpu_offload_gb, enforce_eager, max_seq_len_to_capture, disable_custom_all_reduce, disable_async_output_proc, hf_token, hf_overrides, mm_processor_kwargs, task, override_pooler_config, compilation_config, **kwargs)\u001b[39m\n\u001b[32m    217\u001b[39m engine_args = EngineArgs(\n\u001b[32m    218\u001b[39m     model=model,\n\u001b[32m    219\u001b[39m     task=task,\n\u001b[32m   (...)\u001b[39m\u001b[32m    243\u001b[39m     **kwargs,\n\u001b[32m    244\u001b[39m )\n\u001b[32m    246\u001b[39m \u001b[38;5;66;03m# Create the Engine (autoselects V0 vs V1)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m247\u001b[39m \u001b[38;5;28mself\u001b[39m.llm_engine = \u001b[43mLLMEngine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_engine_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    248\u001b[39m \u001b[43m    \u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43mUsageContext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLLM_CLASS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    249\u001b[39m \u001b[38;5;28mself\u001b[39m.engine_class = \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m.llm_engine)\n\u001b[32m    251\u001b[39m \u001b[38;5;28mself\u001b[39m.request_counter = Counter()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/kure_new/lib/python3.12/site-packages/vllm/engine/llm_engine.py:503\u001b[39m, in \u001b[36mLLMEngine.from_engine_args\u001b[39m\u001b[34m(cls, engine_args, usage_context, stat_loggers)\u001b[39m\n\u001b[32m    501\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Creates an LLM engine from the engine arguments.\"\"\"\u001b[39;00m\n\u001b[32m    502\u001b[39m \u001b[38;5;66;03m# Create the engine configs.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m503\u001b[39m vllm_config = \u001b[43mengine_args\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_engine_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    505\u001b[39m engine_cls = \u001b[38;5;28mcls\u001b[39m\n\u001b[32m    506\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m envs.VLLM_USE_V1:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/kure_new/lib/python3.12/site-packages/vllm/engine/arg_utils.py:1273\u001b[39m, in \u001b[36mEngineArgs.create_engine_config\u001b[39m\u001b[34m(self, usage_context)\u001b[39m\n\u001b[32m   1261\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1262\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInvalid module \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mm\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m in collect_detailed_traces. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1263\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mValid modules are \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mALLOWED_DETAILED_TRACE_MODULES\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m   1264\u001b[39m observability_config = ObservabilityConfig(\n\u001b[32m   1265\u001b[39m     show_hidden_metrics=show_hidden_metrics,\n\u001b[32m   1266\u001b[39m     otlp_traces_endpoint=\u001b[38;5;28mself\u001b[39m.otlp_traces_endpoint,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1270\u001b[39m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mall\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m detailed_trace_modules,\n\u001b[32m   1271\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1273\u001b[39m config = \u001b[43mVllmConfig\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1274\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1275\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1276\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparallel_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparallel_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1277\u001b[39m \u001b[43m    \u001b[49m\u001b[43mscheduler_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscheduler_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1278\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1279\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlora_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlora_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1280\u001b[39m \u001b[43m    \u001b[49m\u001b[43mspeculative_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mspeculative_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1281\u001b[39m \u001b[43m    \u001b[49m\u001b[43mload_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1282\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoding_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoding_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1283\u001b[39m \u001b[43m    \u001b[49m\u001b[43mobservability_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mobservability_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1284\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprompt_adapter_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt_adapter_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1285\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompilation_config\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompilation_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1286\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkv_transfer_config\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkv_transfer_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1287\u001b[39m \u001b[43m    \u001b[49m\u001b[43madditional_config\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43madditional_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1288\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1290\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m config\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<string>:19\u001b[39m, in \u001b[36m__init__\u001b[39m\u001b[34m(self, model_config, cache_config, parallel_config, scheduler_config, device_config, load_config, lora_config, speculative_config, decoding_config, observability_config, prompt_adapter_config, quant_config, compilation_config, kv_transfer_config, additional_config, instance_id)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/kure_new/lib/python3.12/site-packages/vllm/config.py:3848\u001b[39m, in \u001b[36mVllmConfig.__post_init__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   3845\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Verify configs are valid & consistent with each other.\u001b[39;00m\n\u001b[32m   3846\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3847\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3848\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mverify_async_output_proc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparallel_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3849\u001b[39m \u001b[43m                                               \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mspeculative_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3850\u001b[39m \u001b[43m                                               \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3851\u001b[39m     \u001b[38;5;28mself\u001b[39m.model_config.verify_with_parallel_config(\u001b[38;5;28mself\u001b[39m.parallel_config)\n\u001b[32m   3853\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cache_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/kure_new/lib/python3.12/site-packages/vllm/config.py:902\u001b[39m, in \u001b[36mModelConfig.verify_async_output_proc\u001b[39m\u001b[34m(self, parallel_config, speculative_config, device_config)\u001b[39m\n\u001b[32m    899\u001b[39m \u001b[38;5;66;03m# Reminder: Please update docs/source/features/compatibility_matrix.md\u001b[39;00m\n\u001b[32m    900\u001b[39m \u001b[38;5;66;03m# If the feature combo become valid\u001b[39;00m\n\u001b[32m    901\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvllm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mplatforms\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m current_platform\n\u001b[32m--> \u001b[39m\u001b[32m902\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mcurrent_platform\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_async_output_supported\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menforce_eager\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    903\u001b[39m     \u001b[38;5;28mself\u001b[39m.use_async_output_proc = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    904\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/kure_new/lib/python3.12/site-packages/vllm/platforms/interface.py:214\u001b[39m, in \u001b[36mPlatform.is_async_output_supported\u001b[39m\u001b[34m(cls, enforce_eager)\u001b[39m\n\u001b[32m    209\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m    210\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mis_async_output_supported\u001b[39m(\u001b[38;5;28mcls\u001b[39m, enforce_eager: Optional[\u001b[38;5;28mbool\u001b[39m]) -> \u001b[38;5;28mbool\u001b[39m:\n\u001b[32m    211\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    212\u001b[39m \u001b[33;03m    Check if the current platform supports async output.\u001b[39;00m\n\u001b[32m    213\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m214\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n",
      "\u001b[31mNotImplementedError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "if \"mistral\" in model_name.lower() and \"biomistral\" not in model_name.lower():\n",
    "    print(f\"Loading {model_name} with mistral mode...\")\n",
    "\n",
    "\n",
    "    model = LLM(model=path_dir_model, tensor_parallel_size=num_gpus, dtype=\"bfloat16\", seed=seed, max_model_len=args.max_token_all, tokenizer_mode=\"mistral\", load_format=\"mistral\",\n",
    "    config_format=\"mistral\")\n",
    "else:\n",
    "    print(f\"Loading {model_name} ...\")\n",
    "    model = LLM(model=path_dir_model, tensor_parallel_size=num_gpus, dtype=\"bfloat16\", seed=seed, max_model_len=args.max_token_all, gpu_memory_utilization=0.9, enforce_eager=True, device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_params = SamplingParams(seed=seed, temperature=0, max_tokens=args.max_token_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference - test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "data = list_dict_data[idx]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_input= format_chat(\n",
    "    model_name=model_name,\n",
    "    tokenizer=tokenizer,\n",
    "    data=data,\n",
    "    max_token_input=args.max_token_input,\n",
    "    examples=task.examples,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(formatted_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model.generate(formatted_input, sampling_params=sampling_params, use_tqdm=False)\n",
    "for output_one in output:\n",
    "    generated_text = output_one.outputs[0].text\n",
    "    print(f\"Prompt:\\n\\t{formatted_input}\")\n",
    "    print(\"-\"*50)\n",
    "    print(f\"Generated text:\\n\\t{generated_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference - task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sample = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format the input text with the prompt\n",
    "list_input = []\n",
    "list_num_token = []\n",
    "for idx_data, dict_data in enumerate(list_dict_data[:num_sample]):\n",
    "    input_llm = format_chat(\n",
    "        model_name=model_name,\n",
    "        tokenizer=tokenizer,\n",
    "        data=dict_data,\n",
    "        max_token_input=args.max_token_input,\n",
    "        examples=task.examples,\n",
    "    )\n",
    "    list_input.append(input_llm)\n",
    "    len_token_input = len(tokenizer.tokenize(input_llm))\n",
    "    list_num_token.append(len_token_input)\n",
    "    if len_token_input > args.max_token_input:\n",
    "        print(f\"Input exceeds max token limit: id-{idx_data} - {len_token_input} > {args.max_token_input}\")\n",
    "print(f\"Data size: {len(list_input)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list_input[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_stat_num_token = pd.Series(list_num_token).describe().to_dict()\n",
    "print(dict_stat_num_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate how many data will be truncated, max_token_input = max_token_output\n",
    "num_truncate = sum([1 for num_token in list_num_token if num_token > args.max_token_input])\n",
    "proportion_truncate = num_truncate / len(list_num_token)\n",
    "print(f\"The number of data will be truncated: {num_truncate}\")\n",
    "print(f\"The proportion of data will be truncated: {proportion_truncate:.2%}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(list_num_token, bins=30, alpha=0.7)\n",
    "plt.title('Token Count Distribution')\n",
    "plt.xlabel('Number of Tokens')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_pred = []\n",
    "output = model.generate(list_input, sampling_params=sampling_params, use_tqdm=True)\n",
    "for output_one in output:\n",
    "    generated_text = output_one.outputs[0].text\n",
    "    list_pred.append(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for input_text, pred_text in zip(list_input, list_pred):\n",
    "    print(f\"Input text:\\n\\t{input_text}\")\n",
    "    print(\"-\"*50)\n",
    "    print(f\"Generated text:\\n\\t{pred_text}\")\n",
    "    print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx_data, dict_data in enumerate(list_dict_data[:num_sample]):\n",
    "    dict_data[\"pred\"] = list_pred[idx_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_pred_extracted = task.get_pred(list_dict_data[:num_sample], prompt_mode=\"direct\")\n",
    "list_label_extracted = task.get_label(list_dict_data[:num_sample], prompt_mode=\"direct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### list_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_pred_extracted, num_failed = task.get_pred_none(list_pred=list_pred_extracted[:num_sample], list_label=list_label_extracted[:num_sample])\n",
    "print(f\"The number of failed data: {num_failed} ({num_failed/num_sample:.2%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_performance = task.get_performance(list_pred_extracted, list_label_extracted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### list_list_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_pred_extracted, num_failed = task.get_pred_none(list_pred=list_pred_extracted[:num_sample], list_label=list_label_extracted[:num_sample])\n",
    "print(f\"The number of failed data: {num_failed} ({num_failed/num_sample:.2%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_performance, dict_performance_sample = task.get_performance(list_pred_extracted, list_label_extracted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kure_new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
