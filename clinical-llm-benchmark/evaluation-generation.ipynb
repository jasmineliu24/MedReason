{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex\n",
    "import sys\n",
    "import json\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmptyArgs:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "args = EmptyArgs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed everything: 42\n"
     ]
    }
   ],
   "source": [
    "from model.init import seed_everything\n",
    "\n",
    "num_seed = 42\n",
    "seed_everything(seed=num_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metric.generation import calc_metrics_gen, print_metrics_gen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_bootstrap = 1000\n",
    "path_dir_performance = \"performance\"\n",
    "list_prompt_mode = [\"direct\", \"cot\", \"direct-5-shot\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_model = [\n",
    "#     \"Llama-3.3-70B-Instruct\",\n",
    "#     # \"MeLLaMA-70B-chat\",\n",
    "#     \"Mistral-Large-Instruct-2411\",\n",
    "#     \"Phi-3.5-MoE-instruct\",\n",
    "#     \"Yi-1.5-34B-Chat-16K\",\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(task):\n",
    "    dict_prompt_model_performance = {}\n",
    "    for prompt_mode in list_prompt_mode:\n",
    "        dict_model_performance = task.evaluate_by_model(prompt_mode=prompt_mode, bootstrap=num_bootstrap)\n",
    "        # dict_model_performance = task.evaluate_by_model(prompt_mode=prompt_mode, model_name=list_model, bootstrap=num_bootstrap)\n",
    "        path_file_performance = f\"{path_dir_performance}/{task.name}.{prompt_mode}.performance.json\"\n",
    "        with open(path_file_performance, 'w') as f:\n",
    "            json.dump(dict_model_performance, f, indent=4)\n",
    "        dict_prompt_model_performance[prompt_mode] = dict_model_performance\n",
    "    return dict_prompt_model_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_performance(dict_prompt_model_performance):\n",
    "    dict_mode_performance = {}\n",
    "    for prompt_mode in list_prompt_mode:\n",
    "        str_metrics = print_metrics_gen(dict_prompt_model_performance[prompt_mode])\n",
    "        print(\"Prompt Mode:\", prompt_mode)\n",
    "        print(str_metrics)\n",
    "        print(\"===============================\")\n",
    "        dict_mode_performance[prompt_mode] = str_metrics\n",
    "    return dict_mode_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 84.MedDG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset.generation import Task_gen_MedDG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load 84.MedDG data: train: 73530, val: 0, test: 2747\n"
     ]
    }
   ],
   "source": [
    "task = '84.MedDG'\n",
    "task = Task_gen_MedDG(args=args, task=task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [08:27<00:00, 14.92s/it]\n",
      "100%|██████████| 34/34 [11:32<00:00, 20.36s/it]\n",
      "100%|██████████| 34/34 [09:36<00:00, 16.96s/it]\n"
     ]
    }
   ],
   "source": [
    "dict_prompt_model_performance = {}\n",
    "for prompt_mode in list_prompt_mode:\n",
    "    dict_model_performance = task.evaluate_by_model(prompt_mode=prompt_mode, bootstrap=num_bootstrap)\n",
    "    path_file_performance = f\"{path_dir_performance}/{task.name}.{prompt_mode}.performance.json\"\n",
    "    with open(path_file_performance, 'w') as f:\n",
    "        json.dump(dict_model_performance, f, indent=4)\n",
    "    dict_prompt_model_performance[prompt_mode] = dict_model_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.51 [12.50, 12.52]; 22.55 [22.54, 22.57]; 66.49 [66.48, 66.50]; 0.00 [0.00, 0.00]; 12.17 [12.16, 12.18]; 21.72 [21.71, 21.73]; 65.80 [65.80, 65.81]; 0.00 [0.00, 0.00]; 12.45 [12.45, 12.46]; 19.15 [19.14, 19.16]; 65.08 [65.08, 65.09]; 0.00 [0.00, 0.00]; 12.96 [12.96, 12.97]; 20.20 [20.19, 20.21]; 65.55 [65.55, 65.56]; 0.00 [0.00, 0.00]; 9.89 [9.88, 9.90]; 19.62 [19.61, 19.63]; 64.47 [64.46, 64.47]; 0.00 [0.00, 0.00]; 11.05 [11.04, 11.05]; 16.24 [16.23, 16.25]; 63.37 [63.36, 63.37]; 0.00 [0.00, 0.00]; 11.37 [11.36, 11.37]; 16.21 [16.21, 16.22]; 64.05 [64.04, 64.05]; 0.00 [0.00, 0.00]; 8.58 [8.58, 8.58]; 8.80 [8.80, 8.81]; 59.89 [59.88, 59.89]; 0.00 [0.00, 0.00]; 7.73 [7.73, 7.73]; 5.13 [5.12, 5.13]; 53.97 [53.96, 53.98]; 0.00 [0.00, 0.00]; 7.95 [7.95, 7.95]; 5.84 [5.84, 5.85]; 55.35 [55.35, 55.36]; 0.00 [0.00, 0.00]; 10.52 [10.51, 10.53]; 17.01 [17.00, 17.03]; 63.42 [63.42, 63.43]; 0.00 [0.00, 0.00]; 10.73 [10.72, 10.74]; 20.90 [20.88, 20.91]; 64.81 [64.80, 64.82]; 0.00 [0.00, 0.00]; 6.66 [6.66, 6.67]; 2.04 [2.04, 2.05]; 47.65 [47.65, 47.66]; 0.00 [0.00, 0.00]; 12.63 [12.62, 12.63]; 21.79 [21.77, 21.80]; 66.14 [66.13, 66.15]; 0.00 [0.00, 0.00]; 7.31 [7.31, 7.31]; 3.25 [3.24, 3.25]; 53.35 [53.34, 53.35]; 0.00 [0.00, 0.00]; 8.65 [8.65, 8.65]; 8.97 [8.96, 8.97]; 59.88 [59.87, 59.88]; 0.00 [0.00, 0.00]; 7.50 [7.50, 7.51]; 8.50 [8.49, 8.50]; 47.42 [47.39, 47.45]; 21.53 [21.48, 21.58]; 12.61 [12.60, 12.61]; 19.84 [19.83, 19.85]; 65.00 [64.99, 65.00]; 0.00 [0.00, 0.00]; 12.14 [12.14, 12.15]; 18.89 [18.88, 18.90]; 64.56 [64.55, 64.56]; 0.00 [0.00, 0.00]; 11.85 [11.84, 11.86]; 20.96 [20.95, 20.97]; 65.43 [65.43, 65.44]; 0.00 [0.00, 0.00]; 10.70 [10.69, 10.71]; 19.35 [19.34, 19.36]; 64.18 [64.17, 64.19]; 0.00 [0.00, 0.00]; 10.28 [10.27, 10.28]; 13.90 [13.89, 13.90]; 62.40 [62.40, 62.40]; 0.00 [0.00, 0.00]; 8.24 [8.24, 8.25]; 9.47 [9.46, 9.47]; 55.15 [55.14, 55.17]; 5.76 [5.74, 5.79]; 10.32 [10.32, 10.32]; 13.93 [13.92, 13.93]; 63.06 [63.06, 63.07]; 0.00 [0.00, 0.00]; 11.86 [11.85, 11.86]; 17.85 [17.84, 17.86]; 64.09 [64.08, 64.09]; 0.00 [0.00, 0.00]; 11.27 [11.27, 11.28]; 16.10 [16.10, 16.11]; 63.60 [63.60, 63.61]; 0.00 [0.00, 0.00]; 12.15 [12.14, 12.15]; 17.79 [17.78, 17.79]; 64.65 [64.65, 64.66]; 0.00 [0.00, 0.00]; 12.09 [12.09, 12.09]; 17.75 [17.74, 17.75]; 64.80 [64.80, 64.81]; 0.00 [0.00, 0.00]; 9.11 [9.10, 9.11]; 10.02 [10.01, 10.02]; 60.43 [60.43, 60.44]; 0.00 [0.00, 0.00]; 11.61 [11.60, 11.61]; 16.69 [16.69, 16.70]; 64.46 [64.45, 64.47]; 0.00 [0.00, 0.00]; 10.82 [10.82, 10.83]; 15.21 [15.20, 15.22]; 63.23 [63.22, 63.24]; 0.00 [0.00, 0.00]; 11.71 [11.70, 11.71]; 16.97 [16.96, 16.98]; 64.32 [64.32, 64.33]; 0.00 [0.00, 0.00]; 12.99 [12.98, 13.00]; 20.23 [20.22, 20.24]; 65.64 [65.63, 65.64]; 0.00 [0.00, 0.00]; 10.89 [10.89, 10.90]; 15.22 [15.21, 15.23]; 63.73 [63.73, 63.73]; 0.00 [0.00, 0.00];\n"
     ]
    }
   ],
   "source": [
    "str_metrics = print_metrics_gen(dict_prompt_model_performance['direct'])\n",
    "print(str_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.11 [10.10, 10.11]; 12.48 [12.47, 12.49]; 61.39 [61.39, 61.40]; 0.00 [0.00, 0.00]; 9.52 [9.52, 9.53]; 10.35 [10.34, 10.36]; 59.93 [59.93, 59.94]; 0.00 [0.00, 0.00]; 11.62 [11.61, 11.62]; 17.18 [17.17, 17.19]; 64.15 [64.14, 64.15]; 0.00 [0.00, 0.00]; 11.47 [11.47, 11.48]; 16.59 [16.58, 16.59]; 64.15 [64.15, 64.15]; 0.00 [0.00, 0.00]; 7.85 [7.85, 7.85]; 6.35 [6.35, 6.35]; 57.21 [57.20, 57.21]; 0.00 [0.00, 0.00]; 10.59 [10.59, 10.60]; 14.51 [14.50, 14.52]; 62.65 [62.64, 62.65]; 0.00 [0.00, 0.00]; 10.41 [10.41, 10.42]; 14.11 [14.10, 14.11]; 62.80 [62.80, 62.81]; 0.00 [0.00, 0.00]; 7.44 [7.44, 7.45]; 4.42 [4.41, 4.42]; 55.27 [55.27, 55.28]; 0.00 [0.00, 0.00]; 7.84 [7.83, 7.84]; 5.47 [5.46, 5.47]; 54.63 [54.63, 54.64]; 0.00 [0.00, 0.00]; 8.17 [8.17, 8.17]; 6.58 [6.57, 6.58]; 56.39 [56.38, 56.39]; 0.00 [0.00, 0.00]; 10.24 [10.24, 10.25]; 13.26 [13.25, 13.27]; 61.45 [61.44, 61.45]; 0.00 [0.00, 0.00]; 11.02 [11.01, 11.03]; 20.85 [20.84, 20.87]; 64.86 [64.85, 64.86]; 0.00 [0.00, 0.00]; 6.71 [6.71, 6.72]; 2.22 [2.21, 2.22]; 47.76 [47.76, 47.77]; 0.00 [0.00, 0.00]; 12.14 [12.13, 12.15]; 18.42 [18.40, 18.43]; 63.62 [63.61, 63.63]; 0.28 [0.28, 0.29]; 7.23 [7.23, 7.23]; 2.93 [2.92, 2.94]; 50.53 [50.52, 50.54]; 0.00 [0.00, 0.00]; 7.91 [7.91, 7.91]; 5.85 [5.84, 5.85]; 55.14 [55.13, 55.15]; 0.00 [0.00, 0.00]; 8.24 [8.24, 8.25]; 7.21 [7.20, 7.21]; 58.08 [58.08, 58.08]; 0.00 [0.00, 0.00]; 11.66 [11.66, 11.67]; 17.21 [17.21, 17.22]; 64.09 [64.08, 64.09]; 0.00 [0.00, 0.00]; 11.13 [11.12, 11.13]; 16.00 [15.99, 16.00]; 63.47 [63.46, 63.47]; 0.00 [0.00, 0.00]; 12.32 [12.32, 12.33]; 18.39 [18.38, 18.40]; 64.71 [64.71, 64.72]; 0.00 [0.00, 0.00]; 8.09 [8.09, 8.10]; 8.41 [8.40, 8.42]; 56.89 [56.88, 56.90]; 0.00 [0.00, 0.00]; 8.16 [8.16, 8.16]; 7.13 [7.12, 7.13]; 57.54 [57.54, 57.55]; 0.00 [0.00, 0.00]; 7.01 [7.01, 7.02]; 4.25 [4.25, 4.26]; 54.97 [54.96, 54.98]; 0.69 [0.68, 0.70]; 9.42 [9.42, 9.42]; 11.71 [11.70, 11.71]; 61.71 [61.71, 61.72]; 0.00 [0.00, 0.00]; 8.56 [8.56, 8.56]; 9.12 [9.12, 9.13]; 59.47 [59.47, 59.48]; 0.00 [0.00, 0.00]; 10.07 [10.07, 10.07]; 13.18 [13.17, 13.19]; 62.11 [62.10, 62.11]; 0.00 [0.00, 0.00]; 9.71 [9.71, 9.71]; 12.19 [12.19, 12.20]; 61.98 [61.98, 61.99]; 0.00 [0.00, 0.00]; 10.28 [10.28, 10.28]; 13.60 [13.60, 13.61]; 62.90 [62.89, 62.90]; 0.00 [0.00, 0.00]; 8.20 [8.20, 8.20]; 7.09 [7.08, 7.09]; 58.10 [58.09, 58.10]; 0.00 [0.00, 0.00]; 9.70 [9.70, 9.70]; 12.16 [12.16, 12.17]; 62.27 [62.27, 62.28]; 0.00 [0.00, 0.00]; 8.96 [8.96, 8.97]; 9.98 [9.98, 9.99]; 60.56 [60.55, 60.56]; 0.00 [0.00, 0.00]; 9.01 [9.01, 9.01]; 10.18 [10.17, 10.18]; 60.70 [60.70, 60.71]; 0.00 [0.00, 0.00]; 9.89 [9.89, 9.89]; 12.44 [12.44, 12.45]; 61.92 [61.91, 61.92]; 0.00 [0.00, 0.00]; 9.70 [9.70, 9.70]; 12.23 [12.23, 12.24]; 61.77 [61.77, 61.78]; 0.00 [0.00, 0.00];\n"
     ]
    }
   ],
   "source": [
    "str_metrics = print_metrics_gen(dict_prompt_model_performance['cot'])\n",
    "print(str_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.54 [12.53, 12.55]; 22.24 [22.23, 22.25]; 66.67 [66.67, 66.68]; 0.00 [0.00, 0.00]; 12.76 [12.75, 12.77]; 22.20 [22.19, 22.21]; 66.35 [66.34, 66.36]; 0.00 [0.00, 0.00]; 12.66 [12.65, 12.67]; 22.22 [22.21, 22.23]; 66.75 [66.74, 66.75]; 0.00 [0.00, 0.00]; 13.62 [13.61, 13.62]; 23.18 [23.17, 23.19]; 67.26 [67.25, 67.26]; 0.00 [0.00, 0.00]; 9.69 [9.69, 9.70]; 20.21 [20.20, 20.22]; 65.04 [65.04, 65.05]; 0.00 [0.00, 0.00]; 10.37 [10.36, 10.37]; 20.09 [20.08, 20.10]; 66.05 [66.04, 66.06]; 0.00 [0.00, 0.00]; 13.97 [13.97, 13.98]; 21.95 [21.94, 21.96]; 66.65 [66.64, 66.65]; 0.00 [0.00, 0.00]; 8.74 [8.74, 8.74]; 9.20 [9.20, 9.21]; 60.14 [60.14, 60.14]; 0.00 [0.00, 0.00]; 7.74 [7.74, 7.74]; 5.23 [5.23, 5.23]; 54.99 [54.99, 55.00]; 0.00 [0.00, 0.00]; 7.63 [7.63, 7.63]; 4.77 [4.76, 4.77]; 53.90 [53.89, 53.90]; 0.00 [0.00, 0.00]; 0.00 [0.00, 0.00]; 0.00 [0.00, 0.00]; 0.02 [0.02, 0.02]; 99.96 [99.96, 99.96]; 0.00 [0.00, 0.00]; 0.00 [0.00, 0.00]; 0.02 [0.02, 0.02]; 99.96 [99.96, 99.96]; 7.95 [7.95, 7.96]; 5.30 [5.29, 5.31]; 52.74 [52.73, 52.75]; 0.00 [0.00, 0.00]; 13.21 [13.20, 13.22]; 24.89 [24.88, 24.90]; 68.09 [68.08, 68.09]; 0.00 [0.00, 0.00]; 8.49 [8.48, 8.49]; 6.51 [6.50, 6.52]; 55.62 [55.61, 55.62]; 0.00 [0.00, 0.00]; 7.64 [7.64, 7.64]; 5.78 [5.77, 5.78]; 56.74 [56.73, 56.75]; 2.35 [2.34, 2.37]; 9.94 [9.94, 9.94]; 11.36 [11.35, 11.36]; 60.49 [60.49, 60.50]; 0.00 [0.00, 0.00]; 12.43 [12.42, 12.44]; 19.34 [19.33, 19.34]; 64.91 [64.91, 64.92]; 0.00 [0.00, 0.00]; 12.10 [12.10, 12.11]; 21.01 [21.00, 21.02]; 65.69 [65.68, 65.70]; 0.00 [0.00, 0.00]; 13.29 [13.28, 13.29]; 20.92 [20.90, 20.93]; 65.89 [65.88, 65.90]; 0.04 [0.03, 0.04]; 10.67 [10.66, 10.67]; 18.03 [18.02, 18.04]; 64.04 [64.04, 64.05]; 0.00 [0.00, 0.00]; 6.76 [6.76, 6.76]; 1.46 [1.46, 1.47]; 42.97 [42.96, 42.98]; 0.00 [0.00, 0.00]; 7.27 [7.27, 7.28]; 3.28 [3.27, 3.29]; 50.86 [50.85, 50.87]; 0.11 [0.11, 0.12]; 12.09 [12.08, 12.09]; 20.19 [20.18, 20.20]; 65.46 [65.45, 65.46]; 0.00 [0.00, 0.00]; 12.42 [12.42, 12.43]; 20.86 [20.85, 20.87]; 65.32 [65.32, 65.33]; 0.00 [0.00, 0.00]; 13.82 [13.82, 13.83]; 22.15 [22.14, 22.16]; 66.52 [66.51, 66.52]; 0.00 [0.00, 0.00]; 14.24 [14.23, 14.25]; 23.55 [23.54, 23.57]; 67.50 [67.50, 67.51]; 0.00 [0.00, 0.00]; 14.64 [14.63, 14.65]; 24.35 [24.34, 24.37]; 67.85 [67.84, 67.86]; 0.00 [0.00, 0.00]; 13.71 [13.70, 13.72]; 23.73 [23.71, 23.74]; 67.31 [67.30, 67.32]; 0.00 [0.00, 0.00]; 14.54 [14.53, 14.55]; 23.52 [23.51, 23.53]; 67.50 [67.49, 67.50]; 0.00 [0.00, 0.00]; 13.16 [13.15, 13.17]; 19.95 [19.93, 19.96]; 65.58 [65.57, 65.58]; 0.00 [0.00, 0.00]; 13.44 [13.43, 13.45]; 25.30 [25.29, 25.31]; 68.63 [68.62, 68.64]; 0.00 [0.00, 0.00]; 13.95 [13.94, 13.96]; 22.74 [22.73, 22.75]; 66.95 [66.95, 66.96]; 0.00 [0.00, 0.00]; 13.49 [13.49, 13.50]; 20.33 [20.32, 20.34]; 66.02 [66.01, 66.02]; 0.00 [0.00, 0.00];\n"
     ]
    }
   ],
   "source": [
    "str_metrics = print_metrics_gen(dict_prompt_model_performance['direct-5-shot'])\n",
    "print(str_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check wrong format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['result/84.MedDG/Llama-3.1-70B-Instruct/84.MedDG-direct-greedy-42.result.json']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_model_result = task.search_result_by_model()\n",
    "model_one = 'Llama-3.1-70B-Instruct'\n",
    "dict_model_result[model_one]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_one = dict_model_result[model_one][0]\n",
    "with open(result_one, 'r') as f:\n",
    "    list_dict_result = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_label = task.get_label(list_dict_result)\n",
    "list_pred = task.get_pred(list_dict_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bleu': 9.952680117936573,\n",
       " 'rouge': 20.207461132084305,\n",
       " 'meteor': 0.0,\n",
       " 'bertscore': 65.5563459684513}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_metrics_avg, dict_metrics_sample = calc_metrics_gen(list_label, list_pred, lang=task.language)\n",
    "dict_metrics_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 23.cMedQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset.generation import Task_gen_cMedQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load 23.cMedQA data: train: 162517, val: 6132, test: 6184\n",
      "Notice: this task has multiple reference labels\n"
     ]
    }
   ],
   "source": [
    "task = '23.cMedQA'\n",
    "task = Task_gen_cMedQA(args=args, task=task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [55:47<00:00, 98.46s/it] \n",
      "100%|██████████| 34/34 [1:14:17<00:00, 131.10s/it]\n",
      "  9%|▉         | 3/34 [04:11<42:37, 82.48s/it]   "
     ]
    }
   ],
   "source": [
    "dict_prompt_model_performance = evaluate(task)\n",
    "# dict_prompt_model_performance = {}\n",
    "# for prompt_mode in list_prompt_mode:\n",
    "#     path_file_performance = f\"{path_dir_performance}/{task.name}.{prompt_mode}.performance.json\"\n",
    "#     with open(path_file_performance, 'r', encoding='utf-8') as f:\n",
    "#         dict_model_performance = json.load(f)\n",
    "#     dict_prompt_model_performance[prompt_mode] = dict_model_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt Mode: direct\n",
      "7.96 [7.96, 7.97]; 17.30 [17.29, 17.30]; 64.26 [64.26, 64.27]; 0.00 [0.00, 0.00]; 11.32 [11.31, 11.32]; 19.07 [19.07, 19.08]; 64.82 [64.82, 64.83]; 0.00 [0.00, 0.00]; 14.36 [14.36, 14.37]; 19.75 [19.74, 19.75]; 65.26 [65.26, 65.26]; 0.00 [0.00, 0.00]; 15.27 [15.27, 15.28]; 21.20 [21.19, 21.20]; 66.36 [66.36, 66.36]; 0.00 [0.00, 0.00]; 0.98 [0.98, 0.99]; 10.42 [10.41, 10.42]; 54.50 [54.50, 54.51]; 0.00 [0.00, 0.00]; 11.98 [11.97, 11.98]; 18.57 [18.57, 18.58]; 64.73 [64.72, 64.73]; 0.00 [0.00, 0.00]; 15.13 [15.13, 15.14]; 20.69 [20.68, 20.69]; 65.99 [65.99, 65.99]; 0.00 [0.00, 0.00]; 10.16 [10.16, 10.16]; 11.64 [11.63, 11.64]; 61.25 [61.25, 61.25]; 0.00 [0.00, 0.00]; 8.31 [8.31, 8.32]; 6.64 [6.64, 6.64]; 52.72 [52.72, 52.73]; 0.00 [0.00, 0.00]; 8.16 [8.16, 8.16]; 6.10 [6.10, 6.11]; 52.44 [52.43, 52.44]; 0.00 [0.00, 0.00]; 8.83 [8.83, 8.84]; 15.46 [15.46, 15.47]; 62.06 [62.05, 62.06]; 0.00 [0.00, 0.00]; 11.21 [11.21, 11.22]; 17.92 [17.92, 17.93]; 64.24 [64.24, 64.24]; 0.00 [0.00, 0.00]; 5.71 [5.70, 5.71]; 11.39 [11.38, 11.39]; 56.14 [56.13, 56.14]; 0.00 [0.00, 0.00]; 13.04 [13.03, 13.05]; 18.95 [18.94, 18.96]; 64.68 [64.68, 64.69]; 0.00 [0.00, 0.00]; 8.43 [8.43, 8.44]; 5.84 [5.84, 5.85]; 58.44 [58.44, 58.44]; 0.00 [0.00, 0.00]; 11.44 [11.44, 11.45]; 14.35 [14.35, 14.35]; 62.78 [62.78, 62.78]; 0.00 [0.00, 0.00]; 10.52 [10.52, 10.53]; 13.15 [13.14, 13.15]; 52.32 [52.30, 52.33]; 16.98 [16.95, 17.01]; 13.28 [13.28, 13.29]; 20.60 [20.60, 20.61]; 66.22 [66.22, 66.22]; 0.00 [0.00, 0.00]; 14.28 [14.27, 14.28]; 20.64 [20.63, 20.64]; 65.97 [65.97, 65.97]; 0.00 [0.00, 0.00]; 14.76 [14.76, 14.77]; 21.10 [21.10, 21.11]; 66.35 [66.34, 66.35]; 0.00 [0.00, 0.00]; 8.65 [8.65, 8.66]; 15.24 [15.24, 15.25]; 61.35 [61.35, 61.36]; 0.00 [0.00, 0.00]; 13.30 [13.30, 13.31]; 19.96 [19.95, 19.96]; 65.54 [65.54, 65.54]; 0.00 [0.00, 0.00]; 12.02 [12.02, 12.02]; 14.89 [14.89, 14.90]; 62.30 [62.30, 62.30]; 0.00 [0.00, 0.00]; 14.11 [14.10, 14.11]; 19.07 [19.06, 19.07]; 65.14 [65.14, 65.14]; 0.00 [0.00, 0.00]; 13.88 [13.87, 13.89]; 21.25 [21.24, 21.26]; 66.35 [66.35, 66.36]; 0.00 [0.00, 0.00]; 15.13 [15.12, 15.13]; 20.73 [20.73, 20.74]; 65.87 [65.86, 65.87]; 0.00 [0.00, 0.00]; 15.25 [15.24, 15.25]; 20.70 [20.69, 20.70]; 65.85 [65.84, 65.85]; 0.00 [0.00, 0.00]; 15.57 [15.57, 15.58]; 21.57 [21.57, 21.58]; 66.39 [66.39, 66.40]; 0.00 [0.00, 0.00]; 12.03 [12.03, 12.03]; 15.06 [15.05, 15.06]; 63.34 [63.34, 63.34]; 0.00 [0.00, 0.00]; 15.65 [15.64, 15.65]; 21.50 [21.50, 21.51]; 66.41 [66.41, 66.41]; 0.00 [0.00, 0.00]; 14.61 [14.61, 14.62]; 20.14 [20.14, 20.15]; 65.64 [65.64, 65.64]; 0.00 [0.00, 0.00]; 14.88 [14.88, 14.89]; 20.47 [20.47, 20.47]; 65.87 [65.87, 65.87]; 0.00 [0.00, 0.00]; 12.19 [12.18, 12.19]; 20.64 [20.64, 20.65]; 66.48 [66.48, 66.48]; 0.00 [0.00, 0.00]; 15.75 [15.75, 15.76]; 21.28 [21.28, 21.29]; 66.30 [66.30, 66.30]; 0.00 [0.00, 0.00];\n",
      "===============================\n",
      "Prompt Mode: cot\n",
      "9.61 [9.61, 9.61]; 9.54 [9.54, 9.55]; 58.95 [58.95, 58.96]; 0.00 [0.00, 0.00]; 10.75 [10.75, 10.76]; 12.02 [12.02, 12.03]; 60.43 [60.42, 60.43]; 0.00 [0.00, 0.00]; 13.17 [13.17, 13.17]; 17.90 [17.90, 17.90]; 64.02 [64.02, 64.02]; 0.00 [0.00, 0.00]; 14.12 [14.11, 14.12]; 19.12 [19.11, 19.12]; 64.97 [64.97, 64.98]; 0.00 [0.00, 0.00]; 9.65 [9.65, 9.66]; 10.53 [10.53, 10.53]; 60.55 [60.55, 60.56]; 0.00 [0.00, 0.00]; 11.12 [11.12, 11.13]; 13.84 [13.84, 13.85]; 62.33 [62.33, 62.33]; 0.00 [0.00, 0.00]; 14.09 [14.09, 14.09]; 19.16 [19.15, 19.16]; 64.69 [64.69, 64.70]; 0.00 [0.00, 0.00]; 10.46 [10.46, 10.47]; 12.59 [12.59, 12.59]; 60.83 [60.83, 60.84]; 0.00 [0.00, 0.00]; 8.35 [8.35, 8.35]; 6.71 [6.70, 6.71]; 52.55 [52.54, 52.55]; 0.00 [0.00, 0.00]; 8.71 [8.71, 8.72]; 7.48 [7.48, 7.49]; 53.38 [53.37, 53.38]; 0.00 [0.00, 0.00]; 10.17 [10.17, 10.18]; 14.23 [14.23, 14.24]; 60.77 [60.77, 60.78]; 0.00 [0.00, 0.00]; 11.71 [11.71, 11.72]; 17.57 [17.56, 17.57]; 63.91 [63.91, 63.92]; 0.00 [0.00, 0.00]; 8.52 [8.52, 8.53]; 13.07 [13.06, 13.08]; 59.13 [59.13, 59.14]; 0.00 [0.00, 0.00]; 11.41 [11.41, 11.42]; 13.91 [13.91, 13.92]; 59.85 [59.84, 59.85]; 0.00 [0.00, 0.00]; 8.42 [8.42, 8.42]; 6.12 [6.12, 6.12]; 58.39 [58.39, 58.40]; 0.00 [0.00, 0.00]; 10.21 [10.21, 10.22]; 13.00 [13.00, 13.00]; 61.77 [61.77, 61.77]; 0.02 [0.02, 0.02]; 11.97 [11.97, 11.97]; 15.26 [15.25, 15.26]; 62.58 [62.58, 62.59]; 0.00 [0.00, 0.00]; 14.01 [14.01, 14.02]; 19.45 [19.44, 19.45]; 65.01 [65.00, 65.01]; 0.00 [0.00, 0.00]; 14.08 [14.08, 14.09]; 19.45 [19.44, 19.45]; 64.85 [64.85, 64.86]; 0.00 [0.00, 0.00]; 14.79 [14.78, 14.79]; 20.33 [20.33, 20.33]; 65.63 [65.62, 65.63]; 0.00 [0.00, 0.00]; 9.72 [9.72, 9.73]; 14.17 [14.17, 14.18]; 60.39 [60.39, 60.40]; 0.00 [0.00, 0.00]; 10.77 [10.77, 10.78]; 12.88 [12.88, 12.89]; 61.83 [61.82, 61.83]; 0.00 [0.00, 0.00]; 11.10 [11.10, 11.11]; 13.17 [13.16, 13.17]; 62.63 [62.63, 62.63]; 0.00 [0.00, 0.00]; 13.78 [13.78, 13.79]; 18.71 [18.71, 18.72]; 64.88 [64.88, 64.88]; 0.00 [0.00, 0.00]; 13.61 [13.61, 13.62]; 18.08 [18.07, 18.08]; 64.55 [64.54, 64.55]; 0.00 [0.00, 0.00]; 13.91 [13.91, 13.91]; 18.58 [18.58, 18.59]; 64.65 [64.65, 64.65]; 0.00 [0.00, 0.00]; 14.85 [14.84, 14.85]; 19.95 [19.95, 19.95]; 65.56 [65.56, 65.57]; 0.00 [0.00, 0.00]; 14.46 [14.46, 14.47]; 19.33 [19.32, 19.33]; 65.29 [65.29, 65.29]; 0.00 [0.00, 0.00]; 11.06 [11.06, 11.06]; 12.93 [12.92, 12.93]; 62.55 [62.54, 62.55]; 0.00 [0.00, 0.00]; 14.19 [14.19, 14.20]; 18.98 [18.97, 18.98]; 65.12 [65.11, 65.12]; 0.00 [0.00, 0.00]; 13.51 [13.51, 13.51]; 18.20 [18.20, 18.20]; 64.33 [64.33, 64.34]; 0.00 [0.00, 0.00]; 14.08 [14.08, 14.08]; 19.04 [19.03, 19.04]; 64.66 [64.66, 64.66]; 0.00 [0.00, 0.00]; 14.96 [14.96, 14.97]; 20.33 [20.33, 20.34]; 65.59 [65.59, 65.60]; 0.00 [0.00, 0.00]; 14.06 [14.06, 14.07]; 18.51 [18.51, 18.52]; 64.94 [64.94, 64.94]; 0.00 [0.00, 0.00];\n",
      "===============================\n",
      "Prompt Mode: direct-5-shot\n",
      "11.44 [11.44, 11.45]; 20.00 [19.99, 20.00]; 65.54 [65.53, 65.54]; 0.00 [0.00, 0.00]; 10.49 [10.49, 10.50]; 19.12 [19.11, 19.12]; 64.84 [64.84, 64.84]; 0.00 [0.00, 0.00]; 14.66 [14.66, 14.67]; 20.28 [20.28, 20.29]; 65.91 [65.90, 65.91]; 0.00 [0.00, 0.00]; 16.46 [16.45, 16.47]; 22.79 [22.78, 22.79]; 67.31 [67.30, 67.31]; 0.00 [0.00, 0.00]; 11.34 [11.34, 11.35]; 16.35 [16.34, 16.36]; 63.77 [63.77, 63.77]; 0.00 [0.00, 0.00]; 12.03 [12.02, 12.03]; 14.80 [14.79, 14.80]; 62.91 [62.91, 62.92]; 0.00 [0.00, 0.00]; 16.54 [16.53, 16.54]; 22.20 [22.20, 22.20]; 66.97 [66.97, 66.97]; 0.00 [0.00, 0.00]; 10.17 [10.17, 10.17]; 11.60 [11.60, 11.61]; 60.93 [60.92, 60.93]; 0.00 [0.00, 0.00]; 9.08 [9.08, 9.09]; 8.38 [8.37, 8.38]; 56.32 [56.31, 56.32]; 0.00 [0.00, 0.00]; 9.52 [9.52, 9.52]; 9.40 [9.40, 9.40]; 57.08 [57.08, 57.09]; 0.00 [0.00, 0.00]; 0.00 [0.00, 0.00]; 0.00 [0.00, 0.00]; 0.00 [0.00, 0.00]; 100.00 [100.00, 100.00]; 0.00 [0.00, 0.00]; 0.00 [0.00, 0.00]; 0.00 [0.00, 0.00]; 100.00 [100.00, 100.00]; 9.53 [9.53, 9.54]; 9.06 [9.05, 9.06]; 59.08 [59.08, 59.08]; 0.00 [0.00, 0.00]; 15.50 [15.50, 15.51]; 21.25 [21.24, 21.25]; 66.24 [66.24, 66.24]; 0.00 [0.00, 0.00]; 7.95 [7.95, 7.95]; 4.74 [4.74, 4.74]; 57.66 [57.65, 57.66]; 0.00 [0.00, 0.00]; 10.06 [10.06, 10.06]; 11.31 [11.31, 11.32]; 61.55 [61.54, 61.55]; 0.00 [0.00, 0.00]; 13.54 [13.54, 13.54]; 17.22 [17.22, 17.23]; 64.20 [64.20, 64.21]; 0.00 [0.00, 0.00]; 14.64 [14.64, 14.65]; 21.29 [21.29, 21.30]; 66.50 [66.49, 66.50]; 0.00 [0.00, 0.00]; 14.96 [14.95, 14.96]; 20.93 [20.93, 20.94]; 66.08 [66.08, 66.08]; 0.00 [0.00, 0.00]; 16.63 [16.62, 16.64]; 22.16 [22.15, 22.16]; 66.99 [66.98, 66.99]; 0.00 [0.00, 0.00]; 10.17 [10.16, 10.17]; 13.67 [13.67, 13.68]; 60.18 [60.17, 60.18]; 0.00 [0.00, 0.00]; 12.07 [12.07, 12.07]; 15.42 [15.41, 15.42]; 63.32 [63.32, 63.33]; 0.00 [0.00, 0.00]; 7.02 [7.02, 7.03]; 1.86 [1.86, 1.86]; 49.18 [49.18, 49.19]; 0.13 [0.13, 0.13]; 14.67 [14.66, 14.67]; 19.44 [19.43, 19.44]; 65.29 [65.28, 65.29]; 0.00 [0.00, 0.00]; 14.03 [14.03, 14.04]; 19.14 [19.13, 19.15]; 65.29 [65.29, 65.30]; 0.00 [0.00, 0.00]; 15.87 [15.86, 15.87]; 21.57 [21.56, 21.57]; 66.35 [66.35, 66.36]; 0.00 [0.00, 0.00]; 16.44 [16.44, 16.45]; 22.37 [22.37, 22.38]; 66.85 [66.85, 66.85]; 0.00 [0.00, 0.00]; 17.19 [17.18, 17.20]; 22.70 [22.69, 22.70]; 66.98 [66.97, 66.98]; 0.00 [0.00, 0.00]; 15.49 [15.48, 15.50]; 19.83 [19.83, 19.84]; 65.77 [65.76, 65.77]; 0.00 [0.00, 0.00]; 16.66 [16.65, 16.66]; 22.04 [22.04, 22.05]; 66.71 [66.71, 66.72]; 0.00 [0.00, 0.00]; 15.17 [15.17, 15.18]; 19.63 [19.62, 19.63]; 65.65 [65.65, 65.65]; 0.00 [0.00, 0.00]; 16.32 [16.32, 16.33]; 20.85 [20.84, 20.85]; 66.26 [66.26, 66.27]; 0.00 [0.00, 0.00]; 15.91 [15.91, 15.92]; 21.90 [21.89, 21.90]; 66.69 [66.69, 66.70]; 0.00 [0.00, 0.00]; 16.93 [16.92, 16.93]; 22.55 [22.54, 22.55]; 67.16 [67.15, 67.16]; 0.00 [0.00, 0.00];\n",
      "===============================\n"
     ]
    }
   ],
   "source": [
    "dict_mode_performance = print_performance(dict_prompt_model_performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 29.EHRQA.qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset.generation import Task_gen_EHRQA_qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load 29.EHRQA.qa data: train: 38015, val: 4431, test: 5097\n",
      "Notice: this task has multiple reference labels\n"
     ]
    }
   ],
   "source": [
    "task = '29.EHRQA.qa'\n",
    "task = Task_gen_EHRQA_qa(args=args, task=task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_prompt_model_performance = evaluate(task)\n",
    "# dict_prompt_model_performance = {}\n",
    "# for prompt_mode in list_prompt_mode:\n",
    "#     path_file_performance = f\"{path_dir_performance}/{task.name}.{prompt_mode}.performance.json\"\n",
    "#     with open(path_file_performance, 'r', encoding='utf-8') as f:\n",
    "#         dict_model_performance = json.load(f)\n",
    "#     dict_prompt_model_performance[prompt_mode] = dict_model_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt Mode: direct\n",
      "13.63 [13.62, 13.63]; 18.36 [18.35, 18.36]; 65.36 [65.35, 65.36]; 0.00 [0.00, 0.00]; 10.12 [10.12, 10.13]; 10.39 [10.38, 10.40]; 60.75 [60.75, 60.76]; 0.37 [0.36, 0.37]; 13.55 [13.55, 13.56]; 17.94 [17.93, 17.94]; 65.19 [65.19, 65.19]; 0.00 [0.00, 0.00];\n",
      "===============================\n",
      "Prompt Mode: cot\n",
      "12.96 [12.95, 12.96]; 17.41 [17.40, 17.41]; 64.57 [64.56, 64.57]; 0.00 [0.00, 0.00]; 9.61 [9.61, 9.62]; 9.17 [9.16, 9.17]; 60.18 [60.18, 60.19]; 0.31 [0.31, 0.32]; 13.20 [13.20, 13.21]; 17.79 [17.79, 17.80]; 64.80 [64.80, 64.81]; 0.00 [0.00, 0.00];\n",
      "===============================\n",
      "Prompt Mode: direct-5-shot\n",
      "14.64 [14.63, 14.64]; 20.52 [20.52, 20.53]; 66.55 [66.55, 66.56]; 0.00 [0.00, 0.00]; 6.98 [6.98, 6.98]; 1.65 [1.64, 1.65]; 49.42 [49.41, 49.43]; 0.06 [0.06, 0.06]; 14.85 [14.84, 14.86]; 18.98 [18.97, 18.99]; 65.75 [65.75, 65.76]; 0.00 [0.00, 0.00];\n",
      "===============================\n"
     ]
    }
   ],
   "source": [
    "dict_mode_performance = print_performance(dict_prompt_model_performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt Mode: direct\n",
      "7.82 [7.81, 7.83]; 17.79 [17.78, 17.80]; 64.85 [64.84, 64.85]; 0.00 [0.00, 0.00]; 8.17 [8.17, 8.18]; 18.56 [18.56, 18.57]; 65.09 [65.09, 65.10]; 0.00 [0.00, 0.00]; 10.66 [10.66, 10.67]; 17.91 [17.90, 17.91]; 64.84 [64.83, 64.84]; 0.00 [0.00, 0.00]; 10.81 [10.81, 10.82]; 19.07 [19.06, 19.07]; 65.66 [65.65, 65.66]; 0.00 [0.00, 0.00]; 1.89 [1.89, 1.90]; 10.92 [10.91, 10.93]; 55.20 [55.20, 55.21]; 0.00 [0.00, 0.00]; 9.26 [9.26, 9.27]; 17.57 [17.57, 17.58]; 64.63 [64.63, 64.64]; 0.00 [0.00, 0.00]; 11.40 [11.40, 11.41]; 18.55 [18.54, 18.55]; 65.23 [65.23, 65.23]; 0.00 [0.00, 0.00]; 9.93 [9.93, 9.94]; 12.25 [12.24, 12.25]; 61.94 [61.94, 61.95]; 0.00 [0.00, 0.00]; 9.25 [9.25, 9.25]; 9.46 [9.46, 9.46]; 54.29 [54.29, 54.30]; 0.00 [0.00, 0.00]; 8.95 [8.95, 8.95]; 9.08 [9.07, 9.08]; 56.19 [56.18, 56.19]; 0.00 [0.00, 0.00]; 6.14 [6.13, 6.14]; 13.03 [13.03, 13.04]; 61.10 [61.10, 61.11]; 0.00 [0.00, 0.00]; 7.91 [7.90, 7.91]; 15.03 [15.02, 15.04]; 62.45 [62.45, 62.46]; 0.00 [0.00, 0.00]; 5.85 [5.85, 5.86]; 2.08 [2.07, 2.08]; 48.62 [48.62, 48.63]; 0.00 [0.00, 0.00]; 10.10 [10.10, 10.11]; 18.20 [18.19, 18.20]; 64.88 [64.88, 64.89]; 0.00 [0.00, 0.00]; 8.10 [8.10, 8.10]; 5.78 [5.77, 5.78]; 58.36 [58.36, 58.37]; 0.00 [0.00, 0.00]; 10.13 [10.13, 10.14]; 12.70 [12.70, 12.71]; 62.22 [62.21, 62.22]; 0.00 [0.00, 0.00]; 9.19 [9.18, 9.19]; 13.51 [13.50, 13.52]; 56.83 [56.81, 56.84]; 9.80 [9.77, 9.82]; 9.60 [9.59, 9.60]; 18.40 [18.39, 18.40]; 65.28 [65.28, 65.29]; 0.00 [0.00, 0.00]; 9.92 [9.92, 9.93]; 18.60 [18.59, 18.60]; 65.24 [65.23, 65.24]; 0.00 [0.00, 0.00]; 10.34 [10.33, 10.34]; 19.55 [19.54, 19.55]; 66.02 [66.01, 66.02]; 0.00 [0.00, 0.00]; 6.91 [6.90, 6.91]; 13.69 [13.68, 13.69]; 61.35 [61.34, 61.35]; 0.00 [0.00, 0.00]; 9.33 [9.32, 9.33]; 16.65 [16.64, 16.65]; 63.74 [63.73, 63.74]; 0.00 [0.00, 0.00]; 9.58 [9.57, 9.58]; 12.19 [12.19, 12.20]; 61.99 [61.98, 61.99]; 0.17 [0.17, 0.17]; 11.30 [11.29, 11.30]; 17.81 [17.81, 17.82]; 64.90 [64.90, 64.91]; 0.00 [0.00, 0.00]; 10.82 [10.81, 10.82]; 19.67 [19.67, 19.68]; 65.88 [65.88, 65.89]; 0.00 [0.00, 0.00]; 11.48 [11.48, 11.48]; 18.58 [18.57, 18.58]; 65.28 [65.27, 65.28]; 0.00 [0.00, 0.00]; 11.38 [11.37, 11.38]; 18.58 [18.57, 18.58]; 65.26 [65.26, 65.27]; 0.00 [0.00, 0.00]; 10.96 [10.96, 10.97]; 19.69 [19.69, 19.70]; 65.87 [65.86, 65.87]; 0.00 [0.00, 0.00]; 10.54 [10.54, 10.54]; 13.91 [13.90, 13.92]; 62.78 [62.78, 62.78]; 0.00 [0.00, 0.00]; 11.10 [11.09, 11.10]; 19.54 [19.53, 19.54]; 65.79 [65.79, 65.79]; 0.00 [0.00, 0.00]; 10.26 [10.25, 10.26]; 18.24 [18.23, 18.24]; 65.03 [65.03, 65.03]; 0.00 [0.00, 0.00]; 11.39 [11.39, 11.40]; 18.14 [18.13, 18.14]; 65.02 [65.02, 65.02]; 0.00 [0.00, 0.00]; 7.75 [7.75, 7.76]; 18.43 [18.42, 18.43]; 65.34 [65.34, 65.35]; 0.00 [0.00, 0.00]; 11.65 [11.65, 11.66]; 19.05 [19.05, 19.06]; 65.57 [65.57, 65.58]; 0.00 [0.00, 0.00];\n",
      "===============================\n",
      "Prompt Mode: cot\n",
      "10.02 [10.02, 10.03]; 13.44 [13.43, 13.44]; 61.96 [61.96, 61.96]; 0.00 [0.00, 0.00]; 10.73 [10.73, 10.73]; 15.51 [15.50, 15.51]; 63.22 [63.22, 63.23]; 0.00 [0.00, 0.00]; 10.42 [10.41, 10.42]; 16.49 [16.48, 16.49]; 63.83 [63.83, 63.83]; 0.00 [0.00, 0.00]; 11.06 [11.06, 11.07]; 17.64 [17.63, 17.64]; 64.75 [64.75, 64.75]; 0.00 [0.00, 0.00]; 9.68 [9.68, 9.68]; 11.77 [11.76, 11.78]; 61.09 [61.09, 61.10]; 0.00 [0.00, 0.00]; 10.02 [10.01, 10.02]; 13.04 [13.03, 13.04]; 62.00 [61.99, 62.00]; 0.00 [0.00, 0.00]; 10.94 [10.93, 10.94]; 17.47 [17.47, 17.48]; 64.39 [64.39, 64.40]; 0.00 [0.00, 0.00]; 9.99 [9.98, 9.99]; 12.92 [12.92, 12.93]; 61.61 [61.61, 61.61]; 0.00 [0.00, 0.00]; 9.24 [9.23, 9.24]; 9.38 [9.38, 9.39]; 54.19 [54.18, 54.19]; 0.00 [0.00, 0.00]; 8.90 [8.90, 8.91]; 9.41 [9.40, 9.41]; 54.81 [54.80, 54.81]; 0.08 [0.08, 0.08]; 9.31 [9.31, 9.31]; 13.48 [13.47, 13.49]; 61.26 [61.26, 61.27]; 0.00 [0.00, 0.00]; 9.14 [9.14, 9.15]; 14.72 [14.71, 14.72]; 62.18 [62.18, 62.19]; 0.02 [0.02, 0.02]; 6.09 [6.09, 6.09]; 2.53 [2.53, 2.53]; 49.07 [49.07, 49.08]; 0.00 [0.00, 0.00]; 9.23 [9.22, 9.23]; 12.79 [12.78, 12.80]; 59.46 [59.45, 59.46]; 0.00 [0.00, 0.00]; 8.39 [8.39, 8.39]; 6.53 [6.53, 6.54]; 58.54 [58.54, 58.54]; 0.00 [0.00, 0.00]; 9.83 [9.82, 9.83]; 13.40 [13.39, 13.40]; 62.11 [62.10, 62.11]; 0.00 [0.00, 0.00]; 10.26 [10.26, 10.27]; 14.46 [14.46, 14.46]; 62.58 [62.58, 62.58]; 0.02 [0.02, 0.02]; 10.77 [10.76, 10.77]; 17.39 [17.39, 17.40]; 64.37 [64.37, 64.37]; 0.00 [0.00, 0.00]; 10.45 [10.44, 10.45]; 16.62 [16.61, 16.63]; 63.98 [63.97, 63.98]; 0.00 [0.00, 0.00]; 10.77 [10.76, 10.77]; 18.89 [18.88, 18.89]; 65.44 [65.43, 65.44]; 0.00 [0.00, 0.00]; 7.70 [7.70, 7.70]; 10.52 [10.52, 10.53]; 58.52 [58.52, 58.53]; 0.02 [0.02, 0.02]; 10.07 [10.07, 10.07]; 12.24 [12.24, 12.25]; 61.11 [61.11, 61.12]; 0.00 [0.00, 0.00]; 9.35 [9.35, 9.35]; 10.19 [10.19, 10.20]; 60.54 [60.53, 60.54]; 0.37 [0.36, 0.37]; 11.11 [11.11, 11.12]; 17.23 [17.23, 17.24]; 64.45 [64.45, 64.46]; 0.00 [0.00, 0.00]; 10.94 [10.93, 10.94]; 15.12 [15.12, 15.13]; 63.16 [63.16, 63.17]; 0.00 [0.00, 0.00]; 11.23 [11.22, 11.23]; 17.14 [17.13, 17.14]; 64.45 [64.44, 64.45]; 0.00 [0.00, 0.00]; 11.28 [11.28, 11.28]; 17.91 [17.90, 17.92]; 64.86 [64.86, 64.86]; 0.00 [0.00, 0.00]; 11.58 [11.58, 11.59]; 18.20 [18.19, 18.20]; 65.10 [65.09, 65.10]; 0.00 [0.00, 0.00]; 9.29 [9.28, 9.29]; 9.62 [9.62, 9.63]; 61.32 [61.32, 61.33]; 0.00 [0.00, 0.00]; 11.59 [11.59, 11.59]; 17.85 [17.85, 17.86]; 64.94 [64.94, 64.95]; 0.00 [0.00, 0.00]; 10.94 [10.94, 10.94]; 17.56 [17.56, 17.57]; 64.41 [64.41, 64.42]; 0.00 [0.00, 0.00]; 11.02 [11.02, 11.02]; 17.96 [17.95, 17.96]; 64.66 [64.66, 64.66]; 0.00 [0.00, 0.00]; 11.04 [11.04, 11.05]; 18.52 [18.52, 18.53]; 64.90 [64.90, 64.90]; 0.00 [0.00, 0.00]; 10.76 [10.76, 10.77]; 13.99 [13.99, 14.00]; 62.96 [62.96, 62.97]; 0.00 [0.00, 0.00];\n",
      "===============================\n",
      "Prompt Mode: direct-5-shot\n",
      "8.92 [8.91, 8.93]; 19.11 [19.11, 19.12]; 65.76 [65.76, 65.77]; 0.00 [0.00, 0.00]; 9.21 [9.20, 9.22]; 19.28 [19.28, 19.29]; 65.71 [65.71, 65.72]; 0.00 [0.00, 0.00]; 8.59 [8.58, 8.60]; 18.44 [18.44, 18.45]; 65.77 [65.77, 65.78]; 0.00 [0.00, 0.00]; 9.59 [9.58, 9.60]; 20.03 [20.02, 20.04]; 66.61 [66.61, 66.62]; 0.00 [0.00, 0.00]; 9.61 [9.61, 9.62]; 14.63 [14.62, 14.64]; 63.23 [63.23, 63.24]; 0.00 [0.00, 0.00]; 7.63 [7.62, 7.63]; 15.18 [15.18, 15.19]; 63.62 [63.61, 63.62]; 0.00 [0.00, 0.00]; 10.83 [10.83, 10.84]; 19.84 [19.83, 19.84]; 66.29 [66.28, 66.29]; 0.00 [0.00, 0.00]; 10.11 [10.11, 10.11]; 12.56 [12.56, 12.57]; 62.26 [62.26, 62.26]; 0.00 [0.00, 0.00]; 8.64 [8.64, 8.64]; 9.04 [9.03, 9.04]; 57.01 [57.01, 57.01]; 0.00 [0.00, 0.00]; 9.42 [9.42, 9.42]; 10.87 [10.86, 10.87]; 58.85 [58.85, 58.86]; 0.00 [0.00, 0.00]; 0.01 [0.01, 0.01]; 0.02 [0.02, 0.02]; 0.11 [0.11, 0.11]; 99.81 [99.80, 99.81]; 0.01 [0.01, 0.01]; 0.02 [0.02, 0.02]; 0.11 [0.11, 0.12]; 99.81 [99.80, 99.81]; 9.78 [9.78, 9.78]; 11.47 [11.46, 11.47]; 60.75 [60.75, 60.76]; 0.00 [0.00, 0.00]; 9.98 [9.98, 9.99]; 15.48 [15.47, 15.49]; 64.44 [64.44, 64.45]; 0.00 [0.00, 0.00]; 8.19 [8.19, 8.19]; 5.85 [5.84, 5.85]; 58.37 [58.37, 58.38]; 0.00 [0.00, 0.00]; 9.93 [9.93, 9.93]; 12.05 [12.04, 12.05]; 61.99 [61.99, 62.00]; 0.04 [0.04, 0.04]; 10.46 [10.45, 10.46]; 15.51 [15.50, 15.51]; 63.60 [63.60, 63.60]; 0.04 [0.04, 0.04]; 9.44 [9.43, 9.45]; 19.06 [19.06, 19.07]; 65.97 [65.97, 65.98]; 0.00 [0.00, 0.00]; 10.38 [10.38, 10.39]; 18.80 [18.79, 18.81]; 65.49 [65.49, 65.50]; 0.00 [0.00, 0.00]; 10.83 [10.82, 10.83]; 20.14 [20.14, 20.15]; 66.59 [66.59, 66.59]; 0.00 [0.00, 0.00]; 8.45 [8.45, 8.45]; 13.26 [13.25, 13.26]; 60.80 [60.79, 60.80]; 0.00 [0.00, 0.00]; 7.16 [7.15, 7.16]; 3.19 [3.19, 3.20]; 46.01 [46.01, 46.02]; 0.00 [0.00, 0.00]; 6.71 [6.71, 6.71]; 1.44 [1.44, 1.45]; 46.16 [46.16, 46.17]; 0.31 [0.30, 0.31]; 10.14 [10.14, 10.15]; 14.61 [14.60, 14.62]; 63.20 [63.19, 63.20]; 0.00 [0.00, 0.00]; 9.43 [9.42, 9.44]; 19.36 [19.36, 19.37]; 65.98 [65.97, 65.98]; 0.00 [0.00, 0.00]; 9.45 [9.44, 9.45]; 19.60 [19.59, 19.60]; 66.04 [66.04, 66.05]; 0.00 [0.00, 0.00]; 10.65 [10.65, 10.66]; 19.63 [19.62, 19.64]; 65.95 [65.94, 65.95]; 0.00 [0.00, 0.00]; 11.65 [11.64, 11.65]; 20.06 [20.05, 20.06]; 66.12 [66.11, 66.12]; 0.00 [0.00, 0.00]; 10.95 [10.94, 10.95]; 16.34 [16.33, 16.35]; 64.60 [64.60, 64.61]; 0.00 [0.00, 0.00]; 11.77 [11.77, 11.78]; 19.64 [19.63, 19.65]; 65.91 [65.90, 65.91]; 0.00 [0.00, 0.00]; 9.96 [9.95, 9.96]; 12.60 [12.60, 12.61]; 63.33 [63.33, 63.34]; 0.00 [0.00, 0.00]; 11.34 [11.34, 11.35]; 18.11 [18.10, 18.12]; 65.48 [65.48, 65.48]; 0.00 [0.00, 0.00]; 9.41 [9.40, 9.41]; 19.47 [19.46, 19.47]; 66.05 [66.05, 66.05]; 0.00 [0.00, 0.00]; 11.60 [11.60, 11.61]; 20.34 [20.34, 20.35]; 66.56 [66.56, 66.56]; 0.00 [0.00, 0.00];\n",
      "===============================\n"
     ]
    }
   ],
   "source": [
    "dict_mode_performance = print_performance(dict_prompt_model_performance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 86.IMCS-V2-MRG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset.generation import Task_gen_IMCS_V2_MRG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load 86.IMCS-V2-MRG data: train: 2472, val: 0, test: 833\n",
      "Notice: this task has multiple reference labels\n"
     ]
    }
   ],
   "source": [
    "task = '86.IMCS-V2-MRG'\n",
    "task = Task_gen_IMCS_V2_MRG(args=args, task=task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [11:31<00:00, 20.34s/it]\n",
      "100%|██████████| 34/34 [13:31<00:00, 23.85s/it]\n",
      "100%|██████████| 34/34 [08:24<00:00, 14.84s/it]\n"
     ]
    }
   ],
   "source": [
    "dict_prompt_model_performance = evaluate(task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt Mode: direct\n",
      "34.98 [34.96, 35.00]; 43.52 [43.50, 43.54]; 76.56 [76.55, 76.57]; 0.00 [0.00, 0.00]; 35.34 [35.32, 35.36]; 43.87 [43.85, 43.89]; 76.56 [76.55, 76.57]; 0.00 [0.00, 0.00]; 26.96 [26.94, 26.97]; 36.73 [36.71, 36.74]; 73.81 [73.81, 73.82]; 0.00 [0.00, 0.00]; 33.18 [33.16, 33.20]; 41.99 [41.97, 42.01]; 75.90 [75.89, 75.91]; 0.00 [0.00, 0.00]; 23.88 [23.85, 23.90]; 30.21 [30.17, 30.24]; 70.28 [70.27, 70.30]; 0.00 [0.00, 0.00]; 24.51 [24.50, 24.53]; 32.74 [32.73, 32.76]; 72.17 [72.16, 72.17]; 0.00 [0.00, 0.00]; 30.94 [30.92, 30.96]; 39.55 [39.53, 39.56]; 74.69 [74.68, 74.69]; 0.00 [0.00, 0.00]; 16.94 [16.93, 16.95]; 25.59 [25.58, 25.61]; 69.19 [69.18, 69.19]; 0.00 [0.00, 0.00]; 12.16 [12.15, 12.17]; 14.78 [14.77, 14.80]; 61.09 [61.08, 61.10]; 0.00 [0.00, 0.00]; 13.07 [13.06, 13.08]; 16.55 [16.54, 16.57]; 62.65 [62.65, 62.66]; 0.00 [0.00, 0.00]; 24.55 [24.53, 24.57]; 30.76 [30.73, 30.78]; 69.42 [69.41, 69.43]; 0.00 [0.00, 0.00]; 23.63 [23.60, 23.66]; 31.64 [31.61, 31.67]; 71.08 [71.07, 71.10]; 0.00 [0.00, 0.00]; 10.60 [10.58, 10.62]; 8.36 [8.33, 8.38]; 54.22 [54.20, 54.23]; 0.00 [0.00, 0.00]; 35.53 [35.51, 35.55]; 43.98 [43.96, 44.00]; 76.27 [76.26, 76.28]; 0.00 [0.00, 0.00]; 17.66 [17.64, 17.68]; 19.29 [19.26, 19.32]; 67.69 [67.68, 67.70]; 0.00 [0.00, 0.00]; 21.78 [21.77, 21.79]; 29.91 [29.89, 29.93]; 71.47 [71.46, 71.47]; 0.00 [0.00, 0.00]; 26.70 [26.68, 26.72]; 34.55 [34.53, 34.57]; 71.28 [71.25, 71.31]; 3.48 [3.44, 3.52]; 31.31 [31.29, 31.33]; 39.63 [39.61, 39.65]; 74.55 [74.54, 74.56]; 0.00 [0.00, 0.00]; 32.84 [32.82, 32.86]; 41.24 [41.22, 41.26]; 75.26 [75.25, 75.27]; 0.00 [0.00, 0.00]; 32.75 [32.74, 32.77]; 41.44 [41.43, 41.46]; 75.45 [75.45, 75.46]; 0.00 [0.00, 0.00]; 16.16 [16.14, 16.18]; 21.12 [21.10, 21.14]; 66.64 [66.63, 66.65]; 0.00 [0.00, 0.00]; 7.52 [7.52, 7.53]; 2.54 [2.53, 2.55]; 49.89 [49.87, 49.91]; 0.00 [0.00, 0.00]; 12.62 [12.59, 12.64]; 10.32 [10.28, 10.35]; 60.32 [60.29, 60.34]; 0.36 [0.35, 0.37]; 25.80 [25.79, 25.81]; 35.29 [35.28, 35.31]; 73.36 [73.35, 73.36]; 0.00 [0.00, 0.00]; 28.49 [28.47, 28.51]; 37.60 [37.58, 37.62]; 74.08 [74.07, 74.09]; 0.00 [0.00, 0.00]; 27.66 [27.64, 27.67]; 36.03 [36.02, 36.05]; 73.61 [73.60, 73.62]; 0.00 [0.00, 0.00]; 29.81 [29.79, 29.83]; 38.48 [38.46, 38.49]; 74.43 [74.42, 74.44]; 0.00 [0.00, 0.00]; 27.72 [27.70, 27.73]; 36.64 [36.62, 36.66]; 73.99 [73.98, 74.00]; 0.00 [0.00, 0.00]; 23.53 [23.51, 23.54]; 32.57 [32.55, 32.58]; 72.67 [72.67, 72.68]; 0.00 [0.00, 0.00]; 26.17 [26.15, 26.18]; 35.02 [35.00, 35.04]; 73.42 [73.42, 73.43]; 0.00 [0.00, 0.00]; 30.56 [30.55, 30.58]; 39.79 [39.77, 39.80]; 75.16 [75.15, 75.17]; 0.00 [0.00, 0.00]; 34.73 [34.71, 34.75]; 43.63 [43.61, 43.64]; 77.04 [77.03, 77.05]; 0.00 [0.00, 0.00]; 29.44 [29.43, 29.46]; 38.79 [38.77, 38.80]; 74.65 [74.64, 74.65]; 0.00 [0.00, 0.00]; 28.69 [28.68, 28.71]; 38.86 [38.85, 38.88]; 74.98 [74.97, 74.99]; 0.00 [0.00, 0.00];\n",
      "===============================\n",
      "Prompt Mode: cot\n",
      "21.93 [21.92, 21.94]; 32.53 [32.51, 32.55]; 71.93 [71.93, 71.94]; 0.00 [0.00, 0.00]; 22.99 [22.98, 23.01]; 34.36 [34.34, 34.37]; 72.61 [72.60, 72.61]; 0.00 [0.00, 0.00]; 27.11 [27.09, 27.13]; 35.30 [35.29, 35.32]; 72.88 [72.87, 72.88]; 0.00 [0.00, 0.00]; 29.84 [29.83, 29.86]; 39.92 [39.91, 39.94]; 75.14 [75.13, 75.15]; 0.00 [0.00, 0.00]; 14.36 [14.35, 14.37]; 17.33 [17.32, 17.35]; 66.00 [66.00, 66.01]; 0.00 [0.00, 0.00]; 21.67 [21.66, 21.69]; 28.87 [28.86, 28.89]; 70.27 [70.26, 70.28]; 0.00 [0.00, 0.00]; 29.92 [29.90, 29.93]; 38.39 [38.37, 38.41]; 74.28 [74.28, 74.29]; 0.00 [0.00, 0.00]; 15.46 [15.45, 15.47]; 22.85 [22.84, 22.87]; 67.67 [67.67, 67.68]; 0.00 [0.00, 0.00]; 12.42 [12.41, 12.43]; 15.21 [15.19, 15.22]; 61.45 [61.44, 61.46]; 0.00 [0.00, 0.00]; 13.08 [13.07, 13.08]; 16.60 [16.58, 16.61]; 62.68 [62.67, 62.68]; 0.00 [0.00, 0.00]; 24.34 [24.32, 24.37]; 30.51 [30.49, 30.54]; 69.18 [69.17, 69.19]; 0.00 [0.00, 0.00]; 24.19 [24.16, 24.21]; 31.55 [31.53, 31.58]; 70.59 [70.58, 70.60]; 0.00 [0.00, 0.00]; 7.09 [7.08, 7.09]; 2.14 [2.14, 2.15]; 49.34 [49.34, 49.35]; 0.00 [0.00, 0.00]; 21.90 [21.87, 21.93]; 24.88 [24.84, 24.92]; 65.27 [65.25, 65.29]; 0.00 [0.00, 0.00]; 16.36 [16.34, 16.38]; 18.27 [18.25, 18.30]; 65.91 [65.90, 65.92]; 0.00 [0.00, 0.00]; 12.57 [12.55, 12.58]; 12.68 [12.65, 12.70]; 57.86 [57.84, 57.88]; 0.00 [0.00, 0.00]; 12.90 [12.88, 12.91]; 14.13 [14.10, 14.16]; 59.02 [59.00, 59.04]; 0.00 [0.00, 0.00]; 25.20 [25.18, 25.22]; 32.51 [32.49, 32.54]; 71.37 [71.36, 71.38]; 0.00 [0.00, 0.00]; 31.09 [31.07, 31.11]; 39.46 [39.44, 39.48]; 74.62 [74.61, 74.62]; 0.00 [0.00, 0.00]; 28.73 [28.71, 28.74]; 37.41 [37.40, 37.43]; 73.99 [73.98, 73.99]; 0.00 [0.00, 0.00]; 15.63 [15.61, 15.65]; 20.55 [20.53, 20.56]; 66.44 [66.43, 66.45]; 0.00 [0.00, 0.00]; 6.93 [6.93, 6.94]; 1.72 [1.72, 1.72]; 49.34 [49.32, 49.35]; 0.00 [0.00, 0.00]; 12.45 [12.43, 12.47]; 10.58 [10.55, 10.61]; 65.80 [65.78, 65.82]; 0.00 [0.00, 0.00]; 25.28 [25.27, 25.30]; 34.93 [34.91, 34.94]; 73.33 [73.32, 73.33]; 0.00 [0.00, 0.00]; 23.98 [23.96, 24.00]; 31.29 [31.26, 31.31]; 71.63 [71.62, 71.64]; 0.00 [0.00, 0.00]; 23.52 [23.51, 23.54]; 31.26 [31.25, 31.28]; 71.76 [71.75, 71.77]; 0.00 [0.00, 0.00]; 25.59 [25.57, 25.60]; 33.64 [33.63, 33.66]; 72.61 [72.60, 72.62]; 0.00 [0.00, 0.00]; 20.79 [20.78, 20.80]; 28.59 [28.58, 28.61]; 71.39 [71.38, 71.40]; 0.00 [0.00, 0.00]; 16.68 [16.67, 16.70]; 21.16 [21.13, 21.19]; 64.94 [64.91, 64.96]; 0.00 [0.00, 0.00]; 19.38 [19.37, 19.39]; 26.52 [26.51, 26.53]; 70.79 [70.78, 70.80]; 0.00 [0.00, 0.00]; 26.88 [26.86, 26.89]; 35.31 [35.29, 35.33]; 72.44 [72.43, 72.46]; 0.00 [0.00, 0.00]; 29.65 [29.63, 29.67]; 38.91 [38.90, 38.93]; 74.83 [74.82, 74.84]; 0.00 [0.00, 0.00]; 28.72 [28.70, 28.73]; 36.84 [36.82, 36.86]; 73.88 [73.88, 73.89]; 0.00 [0.00, 0.00]; 34.15 [34.14, 34.17]; 43.00 [42.98, 43.01]; 76.62 [76.61, 76.63]; 0.00 [0.00, 0.00];\n",
      "===============================\n",
      "Prompt Mode: direct-5-shot\n",
      "44.78 [44.76, 44.80]; 52.10 [52.08, 52.11]; 80.34 [80.33, 80.35]; 0.00 [0.00, 0.00]; 44.61 [44.59, 44.64]; 51.83 [51.81, 51.85]; 80.36 [80.35, 80.37]; 0.00 [0.00, 0.00]; 49.48 [49.46, 49.51]; 54.97 [54.96, 54.99]; 81.18 [81.17, 81.19]; 0.00 [0.00, 0.00]; 49.16 [49.14, 49.19]; 55.10 [55.09, 55.12]; 81.39 [81.38, 81.40]; 0.00 [0.00, 0.00]; 34.39 [34.35, 34.43]; 38.24 [38.19, 38.28]; 74.34 [74.32, 74.35]; 0.00 [0.00, 0.00]; 45.66 [45.63, 45.68]; 51.55 [51.53, 51.57]; 79.78 [79.77, 79.79]; 0.00 [0.00, 0.00]; 45.61 [45.59, 45.63]; 52.96 [52.95, 52.98]; 80.68 [80.67, 80.69]; 0.00 [0.00, 0.00]; 27.61 [27.60, 27.63]; 36.45 [36.44, 36.47]; 73.92 [73.92, 73.93]; 0.00 [0.00, 0.00]; 12.15 [12.14, 12.16]; 14.78 [14.77, 14.79]; 61.08 [61.07, 61.09]; 0.00 [0.00, 0.00]; 20.47 [20.46, 20.49]; 24.70 [24.68, 24.72]; 68.26 [68.25, 68.27]; 0.00 [0.00, 0.00]; 27.63 [27.61, 27.66]; 33.27 [33.24, 33.30]; 71.32 [71.30, 71.33]; 0.00 [0.00, 0.00]; 34.06 [34.03, 34.10]; 40.89 [40.86, 40.92]; 75.13 [75.12, 75.15]; 0.00 [0.00, 0.00]; 48.28 [48.26, 48.31]; 53.69 [53.67, 53.71]; 80.14 [80.13, 80.15]; 0.00 [0.00, 0.00]; 46.45 [46.43, 46.47]; 53.07 [53.05, 53.09]; 80.45 [80.44, 80.46]; 0.00 [0.00, 0.00]; 26.06 [26.02, 26.10]; 27.14 [27.09, 27.18]; 71.41 [71.40, 71.43]; 0.00 [0.00, 0.00]; 26.07 [26.05, 26.09]; 31.93 [31.90, 31.95]; 72.52 [72.51, 72.53]; 0.00 [0.00, 0.00]; 33.79 [33.78, 33.81]; 40.52 [40.51, 40.53]; 75.69 [75.68, 75.70]; 0.11 [0.11, 0.12]; 42.14 [42.12, 42.16]; 49.06 [49.04, 49.08]; 79.02 [79.01, 79.03]; 0.00 [0.00, 0.00]; 11.53 [11.51, 11.54]; 8.53 [8.51, 8.55]; 66.95 [66.94, 66.96]; 0.00 [0.00, 0.00]; 49.14 [49.12, 49.17]; 55.07 [55.06, 55.09]; 81.62 [81.61, 81.63]; 0.00 [0.00, 0.00]; 16.15 [16.13, 16.17]; 21.11 [21.09, 21.13]; 66.64 [66.63, 66.65]; 0.00 [0.00, 0.00]; 23.15 [23.14, 23.16]; 30.66 [30.64, 30.67]; 71.69 [71.68, 71.69]; 0.00 [0.00, 0.00]; 40.08 [40.05, 40.12]; 45.33 [45.29, 45.36]; 78.40 [78.39, 78.41]; 0.00 [0.00, 0.00]; 45.08 [45.05, 45.11]; 50.60 [50.57, 50.63]; 79.63 [79.62, 79.64]; 0.00 [0.00, 0.00]; 43.55 [43.53, 43.57]; 49.92 [49.90, 49.94]; 79.29 [79.28, 79.30]; 0.00 [0.00, 0.00]; 38.80 [38.78, 38.82]; 46.62 [46.60, 46.63]; 78.33 [78.32, 78.33]; 0.00 [0.00, 0.00]; 42.13 [42.10, 42.15]; 49.63 [49.61, 49.65]; 79.47 [79.46, 79.48]; 0.00 [0.00, 0.00]; 37.34 [37.32, 37.36]; 45.95 [45.93, 45.97]; 77.81 [77.81, 77.82]; 0.00 [0.00, 0.00]; 47.77 [47.75, 47.80]; 54.10 [54.08, 54.11]; 81.23 [81.23, 81.24]; 0.00 [0.00, 0.00]; 34.39 [34.37, 34.41]; 43.28 [43.27, 43.30]; 76.68 [76.67, 76.69]; 0.00 [0.00, 0.00]; 48.93 [48.91, 48.95]; 55.01 [54.99, 55.02]; 81.47 [81.47, 81.48]; 0.00 [0.00, 0.00]; 53.18 [53.16, 53.21]; 58.19 [58.17, 58.20]; 82.66 [82.65, 82.66]; 0.00 [0.00, 0.00]; 39.69 [39.67, 39.70]; 47.98 [47.96, 47.99]; 78.98 [78.97, 78.99]; 0.00 [0.00, 0.00]; 43.73 [43.71, 43.75]; 51.27 [51.25, 51.29]; 80.24 [80.23, 80.24]; 0.00 [0.00, 0.00];\n",
      "===============================\n"
     ]
    }
   ],
   "source": [
    "dict_mode_performance = print_performance(dict_prompt_model_performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 103.icliniq-10k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset.generation import Task_gen_icliniq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load 103.icliniq-10k data: train: 5855, val: 732, test: 733\n"
     ]
    }
   ],
   "source": [
    "task = '103.icliniq-10k'\n",
    "task = Task_gen_icliniq(args=args, task=task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/34 [00:00<?, ?it/s]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  3%|▎         | 1/34 [00:18<10:19, 18.76s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  6%|▌         | 2/34 [00:30<07:43, 14.50s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  9%|▉         | 3/34 [00:38<05:55, 11.47s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 12%|█▏        | 4/34 [00:46<05:04, 10.17s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 15%|█▍        | 5/34 [00:59<05:23, 11.16s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 18%|█▊        | 6/34 [01:10<05:12, 11.15s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 21%|██        | 7/34 [01:18<04:35, 10.21s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 24%|██▎       | 8/34 [01:31<04:45, 10.97s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 26%|██▋       | 9/34 [01:42<04:40, 11.21s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 29%|██▉       | 10/34 [01:59<05:05, 12.74s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 32%|███▏      | 11/34 [02:09<04:39, 12.16s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 35%|███▌      | 12/34 [02:21<04:20, 11.84s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 38%|███▊      | 13/34 [02:38<04:40, 13.38s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 41%|████      | 14/34 [02:54<04:49, 14.45s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 44%|████▍     | 15/34 [03:04<04:06, 12.95s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 47%|████▋     | 16/34 [03:14<03:39, 12.20s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 50%|█████     | 17/34 [03:25<03:18, 11.70s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 53%|█████▎    | 18/34 [03:36<03:04, 11.53s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 56%|█████▌    | 19/34 [03:59<03:45, 15.05s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 59%|█████▉    | 20/34 [04:09<03:08, 13.50s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 62%|██████▏   | 21/34 [04:34<03:37, 16.75s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 65%|██████▍   | 22/34 [04:43<02:55, 14.63s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 68%|██████▊   | 23/34 [04:54<02:27, 13.43s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 71%|███████   | 24/34 [05:11<02:25, 14.53s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 74%|███████▎  | 25/34 [05:24<02:06, 14.11s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 76%|███████▋  | 26/34 [05:39<01:55, 14.48s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 79%|███████▉  | 27/34 [05:49<01:31, 13.04s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 82%|████████▏ | 28/34 [05:58<01:10, 11.74s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 85%|████████▌ | 29/34 [06:08<00:56, 11.28s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 88%|████████▊ | 30/34 [06:19<00:44, 11.16s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 91%|█████████ | 31/34 [06:30<00:33, 11.19s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 94%|█████████▍| 32/34 [06:38<00:20, 10.20s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 97%|█████████▋| 33/34 [06:48<00:10, 10.21s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 34/34 [06:58<00:00, 12.32s/it]\n",
      "  0%|          | 0/34 [00:00<?, ?it/s]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  3%|▎         | 1/34 [00:12<07:06, 12.91s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  6%|▌         | 2/34 [00:25<06:45, 12.68s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  9%|▉         | 3/34 [00:37<06:21, 12.30s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 12%|█▏        | 4/34 [00:47<05:47, 11.57s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 15%|█▍        | 5/34 [00:59<05:35, 11.56s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 18%|█▊        | 6/34 [01:11<05:26, 11.68s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 21%|██        | 7/34 [01:23<05:23, 12.00s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 24%|██▎       | 8/34 [01:36<05:21, 12.35s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 26%|██▋       | 9/34 [01:47<04:55, 11.83s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 29%|██▉       | 10/34 [02:01<05:01, 12.58s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 32%|███▏      | 11/34 [02:13<04:42, 12.30s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 35%|███▌      | 12/34 [02:24<04:21, 11.88s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 38%|███▊      | 13/34 [02:41<04:40, 13.34s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 41%|████      | 14/34 [02:58<04:49, 14.49s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 44%|████▍     | 15/34 [03:07<04:03, 12.80s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 47%|████▋     | 16/34 [03:19<03:45, 12.54s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 50%|█████     | 17/34 [03:29<03:21, 11.87s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 53%|█████▎    | 18/34 [03:42<03:16, 12.29s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 56%|█████▌    | 19/34 [04:02<03:38, 14.56s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 59%|█████▉    | 20/34 [04:15<03:15, 13.96s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 62%|██████▏   | 21/34 [04:31<03:10, 14.69s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 65%|██████▍   | 22/34 [04:42<02:44, 13.71s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 68%|██████▊   | 23/34 [04:53<02:20, 12.80s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 71%|███████   | 24/34 [05:15<02:35, 15.59s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 74%|███████▎  | 25/34 [05:28<02:13, 14.81s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 76%|███████▋  | 26/34 [05:41<01:53, 14.24s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 79%|███████▉  | 27/34 [05:54<01:36, 13.73s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 82%|████████▏ | 28/34 [06:05<01:18, 13.15s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 85%|████████▌ | 29/34 [06:16<01:01, 12.36s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 88%|████████▊ | 30/34 [06:29<00:50, 12.65s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 91%|█████████ | 31/34 [06:43<00:38, 12.85s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 94%|█████████▍| 32/34 [06:54<00:24, 12.49s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 97%|█████████▋| 33/34 [07:06<00:12, 12.15s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 34/34 [07:16<00:00, 12.84s/it]\n",
      "  0%|          | 0/34 [00:00<?, ?it/s]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  3%|▎         | 1/34 [00:17<09:41, 17.62s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  6%|▌         | 2/34 [00:30<07:58, 14.96s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  9%|▉         | 3/34 [00:41<06:40, 12.90s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 12%|█▏        | 4/34 [00:52<06:06, 12.22s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 15%|█▍        | 5/34 [01:04<05:51, 12.13s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 18%|█▊        | 6/34 [01:15<05:30, 11.82s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 21%|██        | 7/34 [01:26<05:11, 11.55s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 24%|██▎       | 8/34 [01:39<05:08, 11.87s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 26%|██▋       | 9/34 [01:50<04:53, 11.76s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 29%|██▉       | 10/34 [02:06<05:11, 12.97s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 32%|███▏      | 11/34 [02:20<05:08, 13.43s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 35%|███▌      | 12/34 [02:32<04:45, 12.98s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 38%|███▊      | 13/34 [02:48<04:52, 13.92s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 41%|████      | 14/34 [03:04<04:50, 14.52s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 44%|████▍     | 15/34 [03:17<04:25, 13.99s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 47%|████▋     | 16/34 [03:31<04:10, 13.90s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 50%|█████     | 17/34 [03:45<03:59, 14.11s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 53%|█████▎    | 18/34 [04:00<03:46, 14.19s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 56%|█████▌    | 19/34 [04:25<04:23, 17.56s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 59%|█████▉    | 20/34 [04:36<03:37, 15.52s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 62%|██████▏   | 21/34 [05:00<03:53, 17.99s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 65%|██████▍   | 22/34 [05:09<03:06, 15.51s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 68%|██████▊   | 23/34 [05:20<02:35, 14.14s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 71%|███████   | 24/34 [05:33<02:18, 13.81s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 74%|███████▎  | 25/34 [05:57<02:30, 16.72s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 76%|███████▋  | 26/34 [06:18<02:25, 18.13s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 79%|███████▉  | 27/34 [06:36<02:05, 17.93s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 82%|████████▏ | 28/34 [06:46<01:33, 15.66s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 85%|████████▌ | 29/34 [06:57<01:11, 14.31s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 88%|████████▊ | 30/34 [07:08<00:53, 13.34s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 91%|█████████ | 31/34 [07:19<00:38, 12.68s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 94%|█████████▍| 32/34 [07:30<00:24, 12.13s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 97%|█████████▋| 33/34 [07:53<00:15, 15.18s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 34/34 [08:03<00:00, 14.22s/it]\n"
     ]
    }
   ],
   "source": [
    "dict_prompt_model_performance = evaluate(task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt Mode: direct\n",
      "5.20 [5.19, 5.21]; 10.72 [10.71, 10.73]; 84.14 [84.14, 84.15]; 0.00 [0.00, 0.00]; 6.75 [6.74, 6.76]; 11.97 [11.96, 11.98]; 84.39 [84.38, 84.39]; 0.00 [0.00, 0.00]; 9.03 [9.03, 9.04]; 12.48 [12.47, 12.49]; 83.25 [83.25, 83.25]; 0.00 [0.00, 0.00]; 9.89 [9.88, 9.89]; 13.92 [13.91, 13.93]; 84.19 [84.19, 84.20]; 0.00 [0.00, 0.00]; 6.03 [6.02, 6.05]; 9.81 [9.79, 9.82]; 82.89 [82.88, 82.89]; 0.00 [0.00, 0.00]; 9.34 [9.34, 9.35]; 12.68 [12.67, 12.69]; 83.41 [83.40, 83.41]; 0.00 [0.00, 0.00]; 10.10 [10.09, 10.10]; 13.83 [13.82, 13.84]; 84.10 [84.10, 84.10]; 0.00 [0.00, 0.00]; 8.56 [8.56, 8.56]; 10.43 [10.42, 10.44]; 81.61 [81.61, 81.62]; 0.00 [0.00, 0.00]; 7.90 [7.90, 7.90]; 6.52 [6.51, 6.53]; 77.07 [77.07, 77.08]; 0.00 [0.00, 0.00]; 7.90 [7.90, 7.91]; 7.00 [6.99, 7.00]; 77.62 [77.61, 77.62]; 0.00 [0.00, 0.00]; 10.21 [10.20, 10.22]; 15.16 [15.15, 15.17]; 84.37 [84.36, 84.37]; 0.00 [0.00, 0.00]; 6.74 [6.73, 6.76]; 10.52 [10.51, 10.54]; 82.23 [82.22, 82.23]; 0.00 [0.00, 0.00]; 8.76 [8.75, 8.77]; 13.14 [13.12, 13.15]; 84.01 [84.01, 84.01]; 0.00 [0.00, 0.00]; 9.96 [9.95, 9.97]; 14.52 [14.51, 14.53]; 84.48 [84.48, 84.48]; 0.00 [0.00, 0.00]; 7.70 [7.70, 7.70]; 5.51 [5.50, 5.52]; 78.59 [78.59, 78.60]; 0.00 [0.00, 0.00]; 8.59 [8.58, 8.59]; 10.08 [10.07, 10.08]; 82.01 [82.01, 82.02]; 0.00 [0.00, 0.00]; 10.07 [10.06, 10.07]; 13.38 [13.37, 13.39]; 83.01 [83.01, 83.02]; 0.00 [0.00, 0.00]; 9.01 [9.00, 9.01]; 13.33 [13.32, 13.34]; 84.50 [84.50, 84.51]; 0.00 [0.00, 0.00]; 9.61 [9.61, 9.62]; 13.60 [13.59, 13.61]; 84.44 [84.44, 84.44]; 0.00 [0.00, 0.00]; 9.39 [9.39, 9.40]; 13.57 [13.56, 13.58]; 84.31 [84.31, 84.31]; 0.00 [0.00, 0.00]; 3.98 [3.97, 4.00]; 10.42 [10.41, 10.43]; 84.45 [84.44, 84.45]; 0.00 [0.00, 0.00]; 9.48 [9.48, 9.49]; 13.40 [13.39, 13.40]; 84.55 [84.55, 84.56]; 0.00 [0.00, 0.00]; 7.69 [7.68, 7.69]; 8.67 [8.66, 8.68]; 82.78 [82.77, 82.78]; 0.00 [0.00, 0.00]; 10.14 [10.14, 10.15]; 13.74 [13.73, 13.75]; 84.28 [84.27, 84.28]; 0.00 [0.00, 0.00]; 6.86 [6.85, 6.86]; 12.62 [12.61, 12.63]; 84.60 [84.60, 84.60]; 0.00 [0.00, 0.00]; 8.76 [8.75, 8.76]; 13.75 [13.74, 13.76]; 84.55 [84.54, 84.55]; 0.00 [0.00, 0.00]; 8.93 [8.92, 8.93]; 13.17 [13.16, 13.18]; 84.44 [84.43, 84.44]; 0.00 [0.00, 0.00]; 10.24 [10.23, 10.25]; 14.32 [14.31, 14.33]; 84.51 [84.51, 84.51]; 0.00 [0.00, 0.00]; 8.30 [8.29, 8.30]; 10.56 [10.55, 10.57]; 82.43 [82.43, 82.44]; 0.00 [0.00, 0.00]; 10.30 [10.29, 10.31]; 14.27 [14.26, 14.28]; 84.43 [84.43, 84.43]; 0.00 [0.00, 0.00]; 9.60 [9.60, 9.61]; 12.42 [12.41, 12.43]; 83.36 [83.36, 83.37]; 0.00 [0.00, 0.00]; 9.12 [9.11, 9.12]; 11.31 [11.30, 11.32]; 82.57 [82.57, 82.58]; 0.00 [0.00, 0.00]; 5.43 [5.42, 5.44]; 12.17 [12.16, 12.18]; 84.79 [84.79, 84.79]; 0.00 [0.00, 0.00]; 10.41 [10.41, 10.42]; 14.17 [14.16, 14.18]; 84.50 [84.49, 84.50]; 0.00 [0.00, 0.00];\n",
      "===============================\n",
      "Prompt Mode: cot\n",
      "9.71 [9.71, 9.72]; 13.25 [13.25, 13.26]; 83.43 [83.43, 83.44]; 0.00 [0.00, 0.00]; 9.73 [9.73, 9.74]; 13.31 [13.31, 13.32]; 83.93 [83.92, 83.93]; 0.00 [0.00, 0.00]; 9.43 [9.42, 9.43]; 13.01 [13.00, 13.02]; 83.70 [83.69, 83.70]; 0.00 [0.00, 0.00]; 10.09 [10.08, 10.09]; 13.58 [13.57, 13.59]; 83.95 [83.95, 83.95]; 0.00 [0.00, 0.00]; 6.32 [6.31, 6.33]; 9.56 [9.55, 9.57]; 82.66 [82.66, 82.67]; 0.00 [0.00, 0.00]; 9.48 [9.47, 9.48]; 12.27 [12.26, 12.28]; 83.01 [83.00, 83.01]; 0.00 [0.00, 0.00]; 10.22 [10.22, 10.23]; 13.79 [13.78, 13.80]; 84.18 [84.18, 84.18]; 0.00 [0.00, 0.00]; 9.04 [9.04, 9.04]; 11.45 [11.44, 11.46]; 82.18 [82.18, 82.18]; 0.00 [0.00, 0.00]; 7.81 [7.81, 7.81]; 6.21 [6.20, 6.22]; 77.16 [77.16, 77.16]; 0.00 [0.00, 0.00]; 7.97 [7.97, 7.97]; 7.03 [7.03, 7.04]; 77.82 [77.82, 77.82]; 0.00 [0.00, 0.00]; 10.66 [10.66, 10.67]; 14.72 [14.71, 14.73]; 83.97 [83.97, 83.97]; 0.00 [0.00, 0.00]; 6.56 [6.55, 6.57]; 10.10 [10.09, 10.12]; 81.29 [81.27, 81.31]; 0.96 [0.94, 0.98]; 9.17 [9.16, 9.17]; 12.60 [12.59, 12.61]; 83.66 [83.65, 83.66]; 0.00 [0.00, 0.00]; 9.64 [9.63, 9.65]; 14.20 [14.19, 14.21]; 84.49 [84.48, 84.49]; 0.00 [0.00, 0.00]; 5.62 [5.61, 5.63]; 4.30 [4.29, 4.31]; 76.83 [76.82, 76.83]; 0.00 [0.00, 0.00]; 9.40 [9.40, 9.41]; 12.39 [12.38, 12.40]; 83.34 [83.33, 83.34]; 0.00 [0.00, 0.00]; 9.98 [9.98, 9.99]; 13.05 [13.04, 13.06]; 82.88 [82.88, 82.88]; 0.00 [0.00, 0.00]; 9.80 [9.80, 9.80]; 13.07 [13.06, 13.08]; 83.47 [83.46, 83.47]; 0.00 [0.00, 0.00]; 10.11 [10.10, 10.11]; 13.85 [13.84, 13.86]; 84.18 [84.17, 84.18]; 0.00 [0.00, 0.00]; 9.95 [9.95, 9.96]; 14.17 [14.16, 14.18]; 84.53 [84.52, 84.53]; 0.00 [0.00, 0.00]; 3.26 [3.25, 3.27]; 5.34 [5.33, 5.36]; 82.90 [82.90, 82.91]; 0.00 [0.00, 0.00]; 9.59 [9.59, 9.60]; 12.23 [12.22, 12.24]; 83.18 [83.17, 83.18]; 0.00 [0.00, 0.00]; 9.22 [9.22, 9.23]; 11.04 [11.03, 11.05]; 82.49 [82.49, 82.50]; 0.00 [0.00, 0.00]; 10.29 [10.28, 10.29]; 13.91 [13.90, 13.92]; 84.30 [84.30, 84.30]; 0.00 [0.00, 0.00]; 9.57 [9.57, 9.58]; 12.93 [12.92, 12.94]; 83.75 [83.75, 83.76]; 0.00 [0.00, 0.00]; 10.08 [10.08, 10.09]; 13.88 [13.87, 13.89]; 84.08 [84.08, 84.09]; 0.00 [0.00, 0.00]; 8.75 [8.75, 8.76]; 13.10 [13.09, 13.11]; 84.27 [84.27, 84.27]; 0.00 [0.00, 0.00]; 9.93 [9.93, 9.94]; 13.41 [13.40, 13.42]; 83.45 [83.44, 83.45]; 0.00 [0.00, 0.00]; 8.16 [8.16, 8.17]; 8.85 [8.84, 8.86]; 81.82 [81.81, 81.82]; 0.00 [0.00, 0.00]; 9.96 [9.95, 9.96]; 13.38 [13.37, 13.39]; 83.50 [83.50, 83.51]; 0.00 [0.00, 0.00]; 9.71 [9.71, 9.71]; 12.61 [12.60, 12.62]; 83.52 [83.52, 83.53]; 0.00 [0.00, 0.00]; 9.74 [9.73, 9.74]; 12.57 [12.56, 12.58]; 83.42 [83.42, 83.42]; 0.00 [0.00, 0.00]; 10.17 [10.17, 10.18]; 14.09 [14.08, 14.10]; 84.25 [84.24, 84.25]; 0.00 [0.00, 0.00]; 10.45 [10.45, 10.45]; 14.23 [14.22, 14.24]; 84.46 [84.45, 84.46]; 0.00 [0.00, 0.00];\n",
      "===============================\n",
      "Prompt Mode: direct-5-shot\n",
      "9.69 [9.68, 9.70]; 14.25 [14.25, 14.26]; 84.18 [84.17, 84.18]; 0.00 [0.00, 0.00]; 10.31 [10.30, 10.32]; 14.65 [14.64, 14.66]; 84.42 [84.42, 84.42]; 0.00 [0.00, 0.00]; 10.84 [10.83, 10.84]; 15.27 [15.25, 15.28]; 84.37 [84.36, 84.37]; 0.00 [0.00, 0.00]; 11.42 [11.42, 11.43]; 16.51 [16.49, 16.52]; 84.90 [84.90, 84.91]; 0.00 [0.00, 0.00]; 9.31 [9.30, 9.32]; 13.85 [13.84, 13.87]; 84.10 [84.09, 84.10]; 0.00 [0.00, 0.00]; 10.43 [10.42, 10.44]; 14.54 [14.53, 14.55]; 84.14 [84.14, 84.15]; 0.00 [0.00, 0.00]; 11.36 [11.36, 11.37]; 15.99 [15.98, 16.00]; 84.78 [84.78, 84.79]; 0.00 [0.00, 0.00]; 8.36 [8.36, 8.37]; 10.36 [10.35, 10.37]; 80.90 [80.90, 80.91]; 0.00 [0.00, 0.00]; 7.81 [7.81, 7.82]; 5.63 [5.63, 5.64]; 78.02 [78.02, 78.03]; 0.00 [0.00, 0.00]; 8.09 [8.09, 8.09]; 6.32 [6.31, 6.33]; 78.69 [78.69, 78.70]; 0.00 [0.00, 0.00]; 8.75 [8.74, 8.75]; 9.26 [9.24, 9.27]; 80.51 [80.51, 80.52]; 0.00 [0.00, 0.00]; 7.90 [7.90, 7.90]; 6.50 [6.49, 6.51]; 78.92 [78.91, 78.92]; 0.00 [0.00, 0.00]; 10.72 [10.71, 10.73]; 15.06 [15.05, 15.07]; 83.07 [83.06, 83.08]; 0.00 [0.00, 0.00]; 10.92 [10.92, 10.93]; 15.31 [15.30, 15.32]; 84.46 [84.45, 84.46]; 0.00 [0.00, 0.00]; 7.02 [7.02, 7.02]; 2.75 [2.75, 2.75]; 77.46 [77.45, 77.46]; 0.00 [0.00, 0.00]; 8.48 [8.47, 8.48]; 9.79 [9.78, 9.80]; 81.90 [81.90, 81.90]; 0.00 [0.00, 0.00]; 10.36 [10.36, 10.36]; 13.68 [13.67, 13.69]; 83.07 [83.06, 83.07]; 0.00 [0.00, 0.00]; 9.19 [9.18, 9.20]; 13.94 [13.93, 13.95]; 84.58 [84.58, 84.59]; 0.00 [0.00, 0.00]; 7.03 [7.02, 7.04]; 11.20 [11.18, 11.22]; 83.18 [83.18, 83.19]; 0.00 [0.00, 0.00]; 9.66 [9.65, 9.66]; 15.10 [15.09, 15.11]; 84.68 [84.68, 84.69]; 0.00 [0.00, 0.00]; 9.05 [9.04, 9.06]; 11.52 [11.51, 11.53]; 82.10 [82.09, 82.11]; 0.00 [0.00, 0.00]; 9.57 [9.56, 9.57]; 12.29 [12.28, 12.30]; 82.81 [82.81, 82.81]; 0.00 [0.00, 0.00]; 8.47 [8.47, 8.48]; 8.26 [8.25, 8.27]; 80.79 [80.78, 80.80]; 0.00 [0.00, 0.00]; 9.18 [9.17, 9.19]; 10.36 [10.35, 10.38]; 81.97 [81.96, 81.98]; 0.00 [0.00, 0.00]; 7.52 [7.51, 7.53]; 11.96 [11.95, 11.98]; 83.43 [83.42, 83.43]; 0.00 [0.00, 0.00]; 10.05 [10.05, 10.06]; 14.62 [14.61, 14.63]; 84.65 [84.64, 84.65]; 0.00 [0.00, 0.00]; 9.82 [9.81, 9.83]; 15.42 [15.41, 15.44]; 84.79 [84.78, 84.79]; 0.00 [0.00, 0.00]; 11.36 [11.35, 11.37]; 16.10 [16.09, 16.11]; 84.77 [84.76, 84.77]; 0.00 [0.00, 0.00]; 10.84 [10.84, 10.85]; 15.19 [15.18, 15.21]; 84.24 [84.24, 84.25]; 0.00 [0.00, 0.00]; 11.22 [11.21, 11.22]; 15.71 [15.70, 15.72]; 84.60 [84.60, 84.60]; 0.00 [0.00, 0.00]; 9.26 [9.26, 9.27]; 9.42 [9.40, 9.43]; 81.09 [81.08, 81.10]; 0.00 [0.00, 0.00]; 9.66 [9.65, 9.67]; 10.68 [10.66, 10.70]; 81.57 [81.57, 81.58]; 0.00 [0.00, 0.00]; 11.01 [11.00, 11.02]; 15.86 [15.85, 15.87]; 84.85 [84.84, 84.85]; 0.00 [0.00, 0.00]; 10.74 [10.74, 10.75]; 14.83 [14.82, 14.84]; 84.63 [84.63, 84.63]; 0.00 [0.00, 0.00];\n",
      "===============================\n"
     ]
    }
   ],
   "source": [
    "dict_mode_performance = print_performance(dict_prompt_model_performance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 104.HealthCareMagic-100k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset.generation import Task_gen_HealthCareMagic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load 104.HealthCareMagic-100k data: train: 89592, val: 11205, test: 11199\n"
     ]
    }
   ],
   "source": [
    "task = '104.HealthCareMagic-100k'\n",
    "task = Task_gen_HealthCareMagic(args=args, task=task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict_prompt_model_performance = evaluate(task)\n",
    "dict_prompt_model_performance = {}\n",
    "for prompt_mode in list_prompt_mode:\n",
    "    path_file_performance = f\"{path_dir_performance}/{task.name}.{prompt_mode}.performance.json\"\n",
    "    with open(path_file_performance, 'r', encoding='utf-8') as f:\n",
    "        dict_model_performance = json.load(f)\n",
    "    dict_prompt_model_performance[prompt_mode] = dict_model_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt Mode: direct\n",
      "4.05 [4.05, 4.05]; 10.62 [10.62, 10.62]; 83.79 [83.79, 83.79]; 0.00 [0.00, 0.00]; 5.45 [5.44, 5.45]; 11.73 [11.73, 11.74]; 84.01 [84.01, 84.01]; 0.00 [0.00, 0.00]; 9.24 [9.24, 9.24]; 12.94 [12.94, 12.95]; 82.87 [82.86, 82.87]; 0.00 [0.00, 0.00]; 10.05 [10.05, 10.05]; 14.30 [14.30, 14.30]; 83.91 [83.91, 83.91]; 0.00 [0.00, 0.00]; 6.69 [6.68, 6.69]; 10.97 [10.97, 10.97]; 82.89 [82.89, 82.89]; 0.00 [0.00, 0.00]; 9.42 [9.42, 9.42]; 13.12 [13.12, 13.12]; 83.07 [83.07, 83.07]; 0.00 [0.00, 0.00]; 10.31 [10.30, 10.31]; 14.43 [14.43, 14.43]; 83.83 [83.83, 83.83]; 0.00 [0.00, 0.00]; 8.70 [8.70, 8.70]; 11.31 [11.30, 11.31]; 81.39 [81.39, 81.39]; 0.00 [0.00, 0.00]; 7.88 [7.88, 7.89]; 6.82 [6.82, 6.83]; 76.57 [76.57, 76.58]; 0.00 [0.00, 0.00]; 7.59 [7.58, 7.59]; 6.35 [6.35, 6.35]; 76.26 [76.26, 76.26]; 0.00 [0.00, 0.00]; 8.50 [8.50, 8.50]; 13.92 [13.92, 13.92]; 84.01 [84.01, 84.02]; 0.00 [0.00, 0.00]; 5.98 [5.98, 5.98]; 10.72 [10.72, 10.73]; 82.75 [82.75, 82.76]; 0.00 [0.00, 0.00]; 7.95 [7.94, 7.95]; 12.75 [12.74, 12.75]; 83.58 [83.57, 83.58]; 0.00 [0.00, 0.00]; 9.85 [9.85, 9.85]; 14.70 [14.69, 14.70]; 84.11 [84.11, 84.11]; 0.00 [0.00, 0.00]; 8.06 [8.06, 8.06]; 6.83 [6.83, 6.84]; 79.29 [79.29, 79.29]; 0.00 [0.00, 0.00]; 8.90 [8.90, 8.90]; 11.26 [11.26, 11.26]; 81.95 [81.95, 81.95]; 0.00 [0.00, 0.00]; 10.08 [10.08, 10.08]; 13.75 [13.75, 13.75]; 82.80 [82.80, 82.80]; 0.00 [0.00, 0.00]; 8.39 [8.39, 8.40]; 13.18 [13.18, 13.18]; 84.11 [84.11, 84.11]; 0.00 [0.00, 0.00]; 9.13 [9.13, 9.13]; 13.69 [13.69, 13.69]; 84.10 [84.10, 84.10]; 0.00 [0.00, 0.00]; 8.22 [8.22, 8.22]; 13.19 [13.19, 13.19]; 84.03 [84.03, 84.04]; 0.00 [0.00, 0.00]; 3.21 [3.20, 3.21]; 9.85 [9.84, 9.85]; 83.85 [83.85, 83.85]; 0.00 [0.00, 0.00]; 8.74 [8.74, 8.74]; 13.33 [13.33, 13.33]; 84.29 [84.29, 84.29]; 0.00 [0.00, 0.00]; 7.34 [7.34, 7.35]; 6.90 [6.90, 6.91]; 81.77 [81.77, 81.77]; 0.00 [0.00, 0.00]; 10.02 [10.02, 10.03]; 14.00 [13.99, 14.00]; 84.05 [84.05, 84.05]; 0.00 [0.00, 0.00]; 5.43 [5.43, 5.44]; 11.79 [11.79, 11.79]; 84.13 [84.13, 84.13]; 0.00 [0.00, 0.00]; 8.12 [8.12, 8.13]; 13.37 [13.36, 13.37]; 84.17 [84.17, 84.17]; 0.00 [0.00, 0.00]; 8.19 [8.19, 8.19]; 12.92 [12.92, 12.92]; 84.09 [84.09, 84.09]; 0.00 [0.00, 0.00]; 9.90 [9.90, 9.90]; 14.43 [14.43, 14.43]; 84.23 [84.22, 84.23]; 0.00 [0.00, 0.00]; 8.35 [8.35, 8.35]; 11.33 [11.32, 11.33]; 82.32 [82.32, 82.32]; 0.00 [0.00, 0.00]; 10.06 [10.06, 10.06]; 14.39 [14.39, 14.39]; 84.16 [84.16, 84.16]; 0.00 [0.00, 0.00]; 9.63 [9.63, 9.63]; 12.96 [12.96, 12.97]; 83.18 [83.18, 83.18]; 0.00 [0.00, 0.00]; 9.54 [9.54, 9.54]; 12.49 [12.49, 12.50]; 82.70 [82.70, 82.70]; 0.00 [0.00, 0.00]; 3.04 [3.04, 3.04]; 10.64 [10.64, 10.64]; 84.03 [84.03, 84.03]; 0.00 [0.00, 0.00]; 10.16 [10.16, 10.16]; 14.32 [14.32, 14.32]; 84.13 [84.12, 84.13]; 0.00 [0.00, 0.00];\n",
      "===============================\n",
      "Prompt Mode: cot\n",
      "9.76 [9.75, 9.76]; 13.73 [13.73, 13.73]; 83.13 [83.13, 83.13]; 0.00 [0.00, 0.00]; 9.59 [9.59, 9.60]; 13.87 [13.87, 13.87]; 83.77 [83.77, 83.77]; 0.00 [0.00, 0.00]; 9.43 [9.43, 9.43]; 13.48 [13.48, 13.48]; 83.38 [83.38, 83.38]; 0.00 [0.00, 0.00]; 10.21 [10.21, 10.21]; 14.11 [14.10, 14.11]; 83.70 [83.70, 83.70]; 0.00 [0.00, 0.00]; 6.03 [6.03, 6.03]; 9.65 [9.65, 9.65]; 82.19 [82.19, 82.19]; 0.00 [0.00, 0.00]; 9.73 [9.73, 9.74]; 13.17 [13.16, 13.17]; 82.87 [82.86, 82.87]; 0.00 [0.00, 0.00]; 10.37 [10.37, 10.37]; 14.28 [14.28, 14.28]; 83.94 [83.94, 83.94]; 0.00 [0.00, 0.00]; 9.28 [9.28, 9.29]; 12.40 [12.40, 12.40]; 82.03 [82.03, 82.03]; 0.00 [0.00, 0.00]; 8.01 [8.01, 8.01]; 6.85 [6.85, 6.85]; 77.03 [77.03, 77.03]; 0.01 [0.01, 0.01]; 7.98 [7.98, 7.98]; 6.96 [6.96, 6.97]; 76.94 [76.94, 76.94]; 0.01 [0.01, 0.01]; 10.25 [10.24, 10.25]; 14.74 [14.74, 14.74]; 83.76 [83.76, 83.76]; 0.00 [0.00, 0.00]; 6.75 [6.75, 6.75]; 11.01 [11.00, 11.01]; 81.13 [81.13, 81.14]; 1.80 [1.79, 1.81]; 8.41 [8.41, 8.41]; 12.33 [12.33, 12.33]; 83.38 [83.38, 83.38]; 0.00 [0.00, 0.00]; 9.04 [9.04, 9.04]; 14.12 [14.12, 14.12]; 84.18 [84.18, 84.19]; 0.00 [0.00, 0.00]; 6.59 [6.59, 6.60]; 5.73 [5.73, 5.73]; 77.65 [77.65, 77.65]; 0.00 [0.00, 0.00]; 9.57 [9.57, 9.58]; 13.03 [13.03, 13.03]; 83.10 [83.10, 83.10]; 0.00 [0.00, 0.00]; 10.21 [10.21, 10.22]; 13.75 [13.75, 13.75]; 82.68 [82.68, 82.68]; 0.00 [0.00, 0.00]; 9.93 [9.93, 9.93]; 13.66 [13.66, 13.66]; 83.17 [83.17, 83.17]; 0.00 [0.00, 0.00]; 10.12 [10.12, 10.12]; 14.35 [14.35, 14.35]; 83.99 [83.99, 83.99]; 0.00 [0.00, 0.00]; 9.54 [9.54, 9.54]; 14.11 [14.11, 14.11]; 84.21 [84.21, 84.21]; 0.00 [0.00, 0.00]; 3.55 [3.55, 3.55]; 6.85 [6.85, 6.86]; 82.81 [82.81, 82.81]; 0.00 [0.00, 0.00]; 9.95 [9.95, 9.95]; 13.26 [13.26, 13.26]; 83.15 [83.15, 83.15]; 0.00 [0.00, 0.00]; 9.57 [9.57, 9.58]; 12.26 [12.26, 12.26]; 82.47 [82.47, 82.47]; 0.00 [0.00, 0.00]; 10.27 [10.27, 10.27]; 14.26 [14.26, 14.26]; 84.09 [84.09, 84.09]; 0.00 [0.00, 0.00]; 9.53 [9.53, 9.53]; 13.30 [13.29, 13.30]; 83.53 [83.53, 83.53]; 0.00 [0.00, 0.00]; 9.87 [9.87, 9.87]; 13.87 [13.87, 13.87]; 83.78 [83.78, 83.78]; 0.00 [0.00, 0.00]; 7.27 [7.26, 7.27]; 12.33 [12.32, 12.33]; 83.86 [83.86, 83.86]; 0.00 [0.00, 0.00]; 10.30 [10.30, 10.31]; 14.32 [14.32, 14.32]; 83.46 [83.46, 83.46]; 0.00 [0.00, 0.00]; 8.18 [8.18, 8.18]; 9.32 [9.32, 9.32]; 81.59 [81.59, 81.59]; 0.00 [0.00, 0.00]; 10.33 [10.33, 10.33]; 14.24 [14.24, 14.24]; 83.49 [83.49, 83.49]; 0.00 [0.00, 0.00]; 10.01 [10.01, 10.01]; 13.51 [13.51, 13.51]; 83.45 [83.45, 83.45]; 0.00 [0.00, 0.00]; 10.05 [10.05, 10.05]; 13.57 [13.56, 13.57]; 83.34 [83.34, 83.34]; 0.00 [0.00, 0.00]; 10.20 [10.20, 10.20]; 14.57 [14.57, 14.58]; 83.94 [83.94, 83.95]; 0.00 [0.00, 0.00]; 10.44 [10.44, 10.44]; 14.52 [14.51, 14.52]; 84.08 [84.08, 84.09]; 0.00 [0.00, 0.00];\n",
      "===============================\n",
      "Prompt Mode: direct-5-shot\n",
      "9.40 [9.40, 9.40]; 13.13 [13.13, 13.13]; 82.73 [82.73, 82.73]; 0.00 [0.00, 0.00]; 9.84 [9.84, 9.84]; 13.33 [13.33, 13.33]; 82.86 [82.86, 82.86]; 0.00 [0.00, 0.00]; 10.66 [10.66, 10.66]; 15.00 [14.99, 15.00]; 84.06 [84.06, 84.06]; 0.00 [0.00, 0.00]; 11.04 [11.04, 11.04]; 15.36 [15.36, 15.36]; 84.24 [84.23, 84.24]; 0.00 [0.00, 0.00]; 7.88 [7.88, 7.88]; 12.02 [12.01, 12.02]; 83.55 [83.55, 83.55]; 0.00 [0.00, 0.00]; 9.51 [9.51, 9.52]; 12.58 [12.57, 12.58]; 82.91 [82.91, 82.91]; 0.00 [0.00, 0.00]; 10.79 [10.79, 10.79]; 14.80 [14.80, 14.80]; 84.04 [84.04, 84.04]; 0.00 [0.00, 0.00]; 8.42 [8.42, 8.43]; 10.74 [10.74, 10.74]; 80.32 [80.32, 80.32]; 0.00 [0.00, 0.00]; 7.95 [7.95, 7.95]; 6.00 [6.00, 6.00]; 78.26 [78.26, 78.26]; 0.00 [0.00, 0.00]; 8.24 [8.24, 8.24]; 7.26 [7.26, 7.26]; 79.03 [79.03, 79.03]; 0.00 [0.00, 0.00]; 0.00 [0.00, 0.00]; 0.00 [0.00, 0.00]; 0.00 [0.00, 0.00]; 100.00 [100.00, 100.00]; 0.00 [0.00, 0.00]; 0.00 [0.00, 0.00]; 0.00 [0.00, 0.00]; 100.00 [100.00, 100.00]; 10.52 [10.52, 10.52]; 14.79 [14.79, 14.79]; 83.63 [83.63, 83.63]; 0.00 [0.00, 0.00]; 10.64 [10.64, 10.65]; 14.78 [14.78, 14.78]; 83.86 [83.86, 83.86]; 0.00 [0.00, 0.00]; 7.29 [7.29, 7.29]; 4.07 [4.06, 4.07]; 77.95 [77.95, 77.95]; 0.00 [0.00, 0.00]; 8.69 [8.69, 8.69]; 10.71 [10.71, 10.72]; 81.70 [81.70, 81.70]; 0.00 [0.00, 0.00]; 9.79 [9.79, 9.79]; 12.90 [12.90, 12.90]; 82.36 [82.36, 82.36]; 0.00 [0.00, 0.00]; 9.66 [9.66, 9.66]; 13.98 [13.97, 13.98]; 83.99 [83.99, 83.99]; 0.00 [0.00, 0.00]; 9.49 [9.49, 9.49]; 14.27 [14.27, 14.27]; 84.24 [84.24, 84.24]; 0.00 [0.00, 0.00]; 9.22 [9.21, 9.22]; 14.02 [14.02, 14.02]; 83.98 [83.98, 83.99]; 0.00 [0.00, 0.00]; 9.26 [9.26, 9.26]; 13.74 [13.74, 13.75]; 83.46 [83.46, 83.46]; 0.01 [0.01, 0.01]; 8.09 [8.09, 8.09]; 7.73 [7.73, 7.73]; 78.54 [78.54, 78.54]; 0.00 [0.00, 0.00]; 6.94 [6.94, 6.94]; 2.82 [2.82, 2.82]; 74.92 [74.91, 74.92]; 0.00 [0.00, 0.00]; 9.25 [9.25, 9.25]; 10.83 [10.82, 10.83]; 82.15 [82.15, 82.15]; 0.00 [0.00, 0.00]; 6.32 [6.32, 6.32]; 9.04 [9.04, 9.05]; 82.24 [82.24, 82.24]; 0.00 [0.00, 0.00]; 10.23 [10.23, 10.23]; 14.22 [14.22, 14.22]; 84.04 [84.04, 84.05]; 0.00 [0.00, 0.00]; 8.39 [8.39, 8.39]; 13.74 [13.74, 13.74]; 84.14 [84.14, 84.14]; 0.00 [0.00, 0.00]; 10.99 [10.99, 11.00]; 15.46 [15.46, 15.47]; 84.16 [84.16, 84.16]; 0.00 [0.00, 0.00]; 9.62 [9.62, 9.63]; 13.04 [13.04, 13.04]; 82.82 [82.82, 82.82]; 0.00 [0.00, 0.00]; 10.70 [10.70, 10.71]; 14.90 [14.89, 14.90]; 83.88 [83.88, 83.88]; 0.00 [0.00, 0.00]; 9.87 [9.87, 9.87]; 12.28 [12.28, 12.29]; 82.31 [82.30, 82.31]; 0.00 [0.00, 0.00]; 10.43 [10.43, 10.43]; 14.06 [14.06, 14.07]; 82.77 [82.77, 82.77]; 0.00 [0.00, 0.00]; 9.30 [9.30, 9.30]; 14.81 [14.81, 14.81]; 84.32 [84.32, 84.32]; 0.00 [0.00, 0.00]; 10.38 [10.38, 10.38]; 14.46 [14.45, 14.46]; 84.08 [84.07, 84.08]; 0.00 [0.00, 0.00];\n",
      "===============================\n"
     ]
    }
   ],
   "source": [
    "dict_mode_performance = print_performance(dict_prompt_model_performance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 107.MIMIC-IV BHC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset.generation import Task_gen_mimic_iv_BHC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load 107.MIMIC-IV BHC data: train: 10, val: 1000, test: 1000\n"
     ]
    }
   ],
   "source": [
    "task = '107.MIMIC-IV BHC'\n",
    "task = Task_gen_mimic_iv_BHC(args=args, task=task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 25%|██▌       | 1/4 [00:52<02:38, 52.85s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 50%|█████     | 2/4 [01:12<01:06, 33.31s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 75%|███████▌  | 3/4 [02:13<00:46, 46.12s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 4/4 [02:34<00:00, 38.74s/it]\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 25%|██▌       | 1/4 [00:19<00:59, 19.69s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 50%|█████     | 2/4 [00:37<00:37, 18.84s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 75%|███████▌  | 3/4 [01:52<00:44, 44.45s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 4/4 [02:10<00:00, 32.65s/it]\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 25%|██▌       | 1/4 [00:21<01:03, 21.26s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 50%|█████     | 2/4 [00:40<00:39, 19.93s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 75%|███████▌  | 3/4 [01:05<00:22, 22.27s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 4/4 [02:45<00:00, 41.50s/it]\n"
     ]
    }
   ],
   "source": [
    "dict_prompt_model_performance = evaluate(task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt Mode: direct\n",
      "5.74 [5.73, 5.75]; 18.52 [18.51, 18.53]; 83.53 [83.52, 83.53]; 0.00 [0.00, 0.00]; 5.98 [5.97, 5.98]; 7.98 [7.97, 8.00]; 77.76 [77.75, 77.77]; 0.00 [0.00, 0.00]; 5.30 [5.29, 5.31]; 16.53 [16.52, 16.54]; 83.07 [83.06, 83.07]; 0.00 [0.00, 0.00];\n",
      "===============================\n",
      "Prompt Mode: cot\n",
      "5.16 [5.15, 5.17]; 17.70 [17.69, 17.71]; 83.32 [83.32, 83.32]; 0.00 [0.00, 0.00]; 7.05 [7.04, 7.05]; 7.62 [7.61, 7.64]; 77.69 [77.69, 77.70]; 0.00 [0.00, 0.00]; 3.56 [3.55, 3.57]; 15.46 [15.46, 15.47]; 83.28 [83.28, 83.28]; 0.00 [0.00, 0.00];\n",
      "===============================\n",
      "Prompt Mode: direct-5-shot\n",
      "6.83 [6.82, 6.84]; 19.56 [19.55, 19.57]; 83.76 [83.76, 83.76]; 0.00 [0.00, 0.00]; 6.29 [6.28, 6.31]; 17.95 [17.94, 17.96]; 83.47 [83.46, 83.47]; 0.00 [0.00, 0.00]; 9.34 [9.33, 9.34]; 9.63 [9.62, 9.64]; 80.70 [80.69, 80.70]; 0.00 [0.00, 0.00];\n",
      "===============================\n"
     ]
    }
   ],
   "source": [
    "dict_mode_performance = print_performance(dict_prompt_model_performance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question-Answering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 91-2.CAS-evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset.generation import Task_gen_CAS_evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load 91-2.CAS.evidence data: train: 20, val: 0, test: 696\n"
     ]
    }
   ],
   "source": [
    "task = \"91-2.CAS.evidence\"\n",
    "task = Task_gen_CAS_evidence(args=args, task=task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/34 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [02:30<00:00,  4.43s/it]\n",
      "100%|██████████| 34/34 [02:21<00:00,  4.15s/it]\n",
      "100%|██████████| 34/34 [02:13<00:00,  3.92s/it]\n"
     ]
    }
   ],
   "source": [
    "dict_prompt_model_performance = evaluate(task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt Mode: direct\n",
      "32.64 [32.59, 32.69]; 45.52 [45.47, 45.57]; 81.09 [81.07, 81.12]; 0.28 [0.27, 0.30]; 33.65 [33.60, 33.69]; 46.80 [46.75, 46.85]; 81.88 [81.86, 81.91]; 0.00 [0.00, 0.00]; 29.20 [29.15, 29.24]; 39.18 [39.13, 39.23]; 79.02 [78.99, 79.04]; 0.00 [0.00, 0.00]; 30.58 [30.53, 30.62]; 41.06 [41.01, 41.11]; 78.94 [78.92, 78.97]; 0.00 [0.00, 0.00]; 7.84 [7.81, 7.86]; 13.96 [13.93, 14.00]; 56.23 [56.18, 56.29]; 37.48 [37.36, 37.59]; 20.67 [20.63, 20.71]; 30.19 [30.14, 30.24]; 73.58 [73.55, 73.61]; 6.63 [6.57, 6.69]; 31.67 [31.62, 31.72]; 43.24 [43.19, 43.29]; 80.22 [80.20, 80.25]; 0.00 [0.00, 0.00]; 28.64 [28.59, 28.68]; 36.46 [36.41, 36.51]; 76.01 [75.98, 76.04]; 0.57 [0.55, 0.59]; 0.00 [0.00, 0.00]; 0.00 [0.00, 0.00]; 16.17 [16.11, 16.22]; 72.43 [72.33, 72.53]; 0.13 [0.12, 0.14]; 0.16 [0.15, 0.17]; 0.54 [0.52, 0.55]; 99.27 [99.25, 99.29]; 0.68 [0.67, 0.69]; 0.84 [0.83, 0.86]; 3.22 [3.19, 3.26]; 95.68 [95.63, 95.73]; 19.59 [19.55, 19.63]; 28.50 [28.45, 28.55]; 66.12 [66.06, 66.18]; 15.53 [15.44, 15.61]; 0.00 [0.00, 0.00]; 0.00 [0.00, 0.00]; 0.03 [0.02, 0.03]; 100.00 [100.00, 100.00]; 16.96 [16.92, 17.01]; 23.35 [23.30, 23.40]; 57.32 [57.26, 57.38]; 33.78 [33.67, 33.90]; 4.71 [4.68, 4.74]; 7.24 [7.20, 7.28]; 42.46 [42.39, 42.54]; 35.43 [35.32, 35.55]; 15.39 [15.35, 15.42]; 17.05 [17.01, 17.09]; 67.36 [67.32, 67.39]; 4.88 [4.83, 4.94]; 29.54 [29.49, 29.58]; 36.88 [36.84, 36.93]; 75.69 [75.66, 75.71]; 0.00 [0.00, 0.00]; 27.01 [26.96, 27.05]; 36.21 [36.17, 36.26]; 77.32 [77.29, 77.34]; 0.00 [0.00, 0.00]; 26.68 [26.64, 26.73]; 36.91 [36.86, 36.96]; 77.45 [77.42, 77.47]; 0.00 [0.00, 0.00]; 28.71 [28.67, 28.76]; 40.90 [40.85, 40.95]; 79.38 [79.35, 79.40]; 0.14 [0.13, 0.15]; 0.62 [0.61, 0.62]; 2.10 [2.09, 2.12]; 24.05 [24.04, 24.06]; 100.00 [100.00, 100.00]; 2.50 [2.49, 2.52]; 4.13 [4.10, 4.15]; 11.79 [11.73, 11.85]; 89.80 [89.73, 89.87]; 20.54 [20.49, 20.58]; 27.47 [27.42, 27.52]; 67.09 [67.04, 67.14]; 16.88 [16.79, 16.97]; 30.07 [30.03, 30.12]; 35.61 [35.56, 35.65]; 75.35 [75.32, 75.37]; 0.00 [0.00, 0.00]; 10.06 [10.03, 10.09]; 11.85 [11.82, 11.88]; 68.80 [68.78, 68.81]; 1.14 [1.12, 1.17]; 7.14 [7.11, 7.17]; 8.60 [8.57, 8.63]; 67.88 [67.87, 67.90]; 0.00 [0.00, 0.00]; 26.56 [26.51, 26.62]; 30.33 [30.28, 30.38]; 74.89 [74.87, 74.92]; 0.43 [0.42, 0.45]; 37.80 [37.75, 37.85]; 47.08 [47.02, 47.13]; 80.97 [80.94, 80.99]; 0.00 [0.00, 0.00]; 31.69 [31.64, 31.74]; 38.84 [38.79, 38.89]; 77.06 [77.03, 77.09]; 0.70 [0.68, 0.72]; 37.70 [37.65, 37.76]; 47.33 [47.28, 47.39]; 81.26 [81.23, 81.28]; 0.00 [0.00, 0.00]; 20.61 [20.57, 20.65]; 30.57 [30.52, 30.62]; 75.31 [75.29, 75.33]; 0.00 [0.00, 0.00]; 27.14 [27.09, 27.18]; 34.54 [34.49, 34.59]; 77.09 [77.06, 77.11]; 0.00 [0.00, 0.00]; 20.44 [20.41, 20.47]; 32.28 [32.24, 32.32]; 75.96 [75.94, 75.98]; 0.15 [0.14, 0.16]; 38.39 [38.34, 38.44]; 49.24 [49.19, 49.29]; 82.25 [82.23, 82.27]; 0.00 [0.00, 0.00];\n",
      "===============================\n",
      "Prompt Mode: cot\n",
      "29.32 [29.27, 29.37]; 39.46 [39.41, 39.52]; 78.84 [78.81, 78.87]; 0.15 [0.14, 0.16]; 31.55 [31.51, 31.60]; 39.40 [39.35, 39.44]; 77.23 [77.21, 77.26]; 0.00 [0.00, 0.00]; 29.18 [29.13, 29.22]; 37.52 [37.47, 37.57]; 77.13 [77.11, 77.16]; 0.43 [0.41, 0.44]; 23.46 [23.41, 23.50]; 28.06 [28.00, 28.11]; 60.43 [60.35, 60.50]; 20.25 [20.16, 20.35]; 3.07 [3.06, 3.08]; 3.53 [3.51, 3.54]; 58.87 [58.84, 58.91]; 10.07 [10.00, 10.14]; 17.80 [17.76, 17.84]; 23.33 [23.28, 23.38]; 66.45 [66.40, 66.50]; 8.91 [8.85, 8.98]; 30.29 [30.25, 30.34]; 37.84 [37.79, 37.89]; 77.06 [77.04, 77.08]; 0.00 [0.00, 0.00]; 20.08 [20.05, 20.11]; 22.69 [22.65, 22.73]; 66.20 [66.16, 66.23]; 4.72 [4.67, 4.77]; 0.00 [0.00, 0.00]; 0.00 [0.00, 0.00]; 9.32 [9.27, 9.37]; 84.25 [84.17, 84.34]; 0.00 [0.00, 0.00]; 0.00 [0.00, 0.00]; 0.08 [0.08, 0.09]; 99.85 [99.84, 99.86]; 2.37 [2.35, 2.39]; 3.25 [3.22, 3.27]; 16.43 [16.36, 16.50]; 76.28 [76.17, 76.38]; 22.06 [22.01, 22.10]; 31.56 [31.51, 31.61]; 71.07 [71.02, 71.12]; 9.49 [9.42, 9.56]; 0.00 [0.00, 0.00]; 0.00 [0.00, 0.00]; 0.17 [0.16, 0.17]; 100.00 [100.00, 100.00]; 9.77 [9.73, 9.80]; 14.13 [14.09, 14.17]; 45.11 [45.05, 45.17]; 56.84 [56.72, 56.95]; 8.73 [8.69, 8.77]; 11.60 [11.56, 11.65]; 45.75 [45.68, 45.83]; 35.07 [34.96, 35.18]; 3.87 [3.85, 3.88]; 4.13 [4.11, 4.15]; 35.98 [35.90, 36.06]; 45.87 [45.76, 45.99]; 8.15 [8.13, 8.18]; 8.39 [8.36, 8.42]; 65.50 [65.49, 65.52]; 0.14 [0.13, 0.15]; 27.60 [27.55, 27.65]; 32.63 [32.58, 32.68]; 74.24 [74.22, 74.27]; 0.13 [0.13, 0.14]; 28.99 [28.95, 29.04]; 36.36 [36.31, 36.41]; 76.07 [76.04, 76.09]; 0.00 [0.00, 0.00]; 33.02 [32.97, 33.07]; 42.92 [42.87, 42.97]; 79.27 [79.24, 79.29]; 0.00 [0.00, 0.00]; 0.00 [0.00, 0.00]; 0.00 [0.00, 0.00]; 0.04 [0.04, 0.04]; 100.00 [100.00, 100.00]; 9.03 [9.00, 9.06]; 11.54 [11.50, 11.57]; 45.89 [45.82, 45.96]; 48.69 [48.57, 48.81]; 3.63 [3.61, 3.66]; 4.24 [4.21, 4.27]; 12.32 [12.26, 12.38]; 82.95 [82.86, 83.03]; 28.40 [28.36, 28.45]; 33.35 [33.30, 33.39]; 74.30 [74.28, 74.32]; 0.14 [0.13, 0.15]; 9.99 [9.96, 10.02]; 13.11 [13.07, 13.14]; 66.68 [66.64, 66.72]; 4.61 [4.57, 4.66]; 12.66 [12.63, 12.70]; 18.25 [18.21, 18.29]; 70.97 [70.95, 70.99]; 0.72 [0.70, 0.74]; 23.77 [23.72, 23.81]; 25.90 [25.85, 25.94]; 73.90 [73.88, 73.92]; 0.00 [0.00, 0.00]; 26.03 [25.97, 26.08]; 31.26 [31.20, 31.31]; 59.09 [59.01, 59.17]; 24.00 [23.90, 24.11]; 17.35 [17.30, 17.39]; 20.64 [20.60, 20.69]; 60.48 [60.42, 60.53]; 25.52 [25.41, 25.62]; 26.69 [26.63, 26.74]; 33.03 [32.96, 33.09]; 59.80 [59.71, 59.88]; 24.83 [24.73, 24.93]; 14.51 [14.47, 14.55]; 17.90 [17.86, 17.94]; 69.85 [69.83, 69.88]; 1.14 [1.11, 1.16]; 23.32 [23.27, 23.37]; 29.77 [29.72, 29.82]; 74.56 [74.54, 74.59]; 0.00 [0.00, 0.00]; 19.67 [19.63, 19.70]; 25.72 [25.68, 25.76]; 72.37 [72.35, 72.39]; 1.44 [1.41, 1.46]; 32.83 [32.78, 32.87]; 38.90 [38.86, 38.95]; 76.62 [76.60, 76.64]; 0.00 [0.00, 0.00];\n",
      "===============================\n",
      "Prompt Mode: direct-5-shot\n",
      "42.65 [42.59, 42.71]; 53.91 [53.85, 53.97]; 84.50 [84.48, 84.53]; 0.29 [0.28, 0.30]; 43.55 [43.48, 43.61]; 55.36 [55.30, 55.42]; 85.42 [85.39, 85.45]; 0.14 [0.14, 0.15]; 39.24 [39.18, 39.31]; 49.73 [49.67, 49.79]; 83.35 [83.32, 83.37]; 0.44 [0.42, 0.45]; 43.15 [43.09, 43.21]; 53.74 [53.68, 53.80]; 84.76 [84.73, 84.79]; 0.28 [0.27, 0.30]; 20.24 [20.20, 20.29]; 29.27 [29.22, 29.32]; 74.96 [74.94, 74.99]; 6.05 [6.00, 6.11]; 35.45 [35.39, 35.50]; 47.21 [47.16, 47.27]; 82.77 [82.75, 82.79]; 0.72 [0.70, 0.74]; 41.52 [41.46, 41.59]; 51.90 [51.84, 51.95]; 84.19 [84.16, 84.21]; 0.14 [0.13, 0.15]; 35.03 [34.98, 35.09]; 44.15 [44.10, 44.21]; 78.01 [77.97, 78.04]; 10.76 [10.69, 10.84]; 1.84 [1.82, 1.86]; 2.25 [2.23, 2.27]; 26.04 [25.97, 26.12]; 58.90 [58.78, 59.02]; 30.67 [30.61, 30.73]; 41.82 [41.76, 41.88]; 80.12 [80.08, 80.15]; 2.15 [2.12, 2.19]; 3.88 [3.86, 3.90]; 11.35 [11.31, 11.39]; 51.81 [51.73, 51.88]; 26.74 [26.63, 26.84]; 22.40 [22.34, 22.46]; 32.43 [32.36, 32.50]; 61.04 [60.95, 61.13]; 26.47 [26.37, 26.58]; 36.27 [36.21, 36.33]; 45.32 [45.26, 45.38]; 80.75 [80.72, 80.78]; 0.44 [0.43, 0.46]; 41.01 [40.95, 41.07]; 51.60 [51.54, 51.66]; 83.69 [83.67, 83.72]; 0.14 [0.13, 0.15]; 36.96 [36.90, 37.01]; 46.96 [46.90, 47.02]; 81.28 [81.25, 81.30]; 0.14 [0.13, 0.15]; 30.88 [30.82, 30.93]; 39.20 [39.15, 39.26]; 75.57 [75.52, 75.62]; 5.19 [5.14, 5.24]; 39.91 [39.85, 39.97]; 50.29 [50.23, 50.35]; 83.37 [83.35, 83.40]; 0.13 [0.13, 0.14]; 20.78 [20.74, 20.81]; 23.22 [23.18, 23.25]; 72.77 [72.75, 72.79]; 0.00 [0.00, 0.00]; 37.70 [37.65, 37.76]; 48.73 [48.67, 48.78]; 83.25 [83.22, 83.27]; 0.00 [0.00, 0.00]; 42.74 [42.68, 42.80]; 53.73 [53.67, 53.79]; 84.55 [84.52, 84.58]; 0.14 [0.14, 0.15]; 0.71 [0.70, 0.71]; 2.02 [2.00, 2.03]; 23.92 [23.91, 23.93]; 100.00 [100.00, 100.00]; 21.48 [21.44, 21.52]; 31.99 [31.94, 32.04]; 74.12 [74.08, 74.16]; 5.03 [4.98, 5.08]; 31.44 [31.39, 31.49]; 41.13 [41.07, 41.18]; 79.92 [79.89, 79.94]; 0.42 [0.41, 0.44]; 42.98 [42.92, 43.04]; 53.43 [53.37, 53.49]; 84.39 [84.36, 84.41]; 0.14 [0.13, 0.15]; 21.84 [21.80, 21.89]; 31.57 [31.52, 31.62]; 76.29 [76.26, 76.31]; 0.15 [0.14, 0.15]; 29.43 [29.37, 29.49]; 34.59 [34.53, 34.65]; 77.77 [77.74, 77.79]; 0.00 [0.00, 0.00]; 35.68 [35.63, 35.74]; 45.76 [45.70, 45.81]; 82.31 [82.29, 82.34]; 0.00 [0.00, 0.00]; 44.71 [44.65, 44.77]; 55.07 [55.01, 55.13]; 85.04 [85.01, 85.07]; 0.00 [0.00, 0.00]; 42.33 [42.27, 42.40]; 52.99 [52.93, 53.05]; 84.97 [84.94, 85.00]; 0.00 [0.00, 0.00]; 44.86 [44.80, 44.92]; 55.41 [55.35, 55.47]; 85.18 [85.16, 85.21]; 0.00 [0.00, 0.00]; 40.19 [40.13, 40.25]; 49.41 [49.35, 49.47]; 82.42 [82.40, 82.45]; 0.28 [0.27, 0.30]; 41.38 [41.32, 41.44]; 50.92 [50.86, 50.98]; 83.42 [83.39, 83.45]; 0.15 [0.14, 0.16]; 39.08 [39.02, 39.14]; 51.45 [51.39, 51.51]; 83.94 [83.91, 83.97]; 0.14 [0.13, 0.15]; 42.29 [42.23, 42.34]; 55.87 [55.82, 55.93]; 86.05 [86.02, 86.07]; 0.00 [0.00, 0.00];\n",
      "===============================\n"
     ]
    }
   ],
   "source": [
    "dict_mode_performance = print_performance(dict_prompt_model_performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
